{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_classification_SE_completo.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49e30e29e23a4dba9a83e5ab0ca2ce9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d836320b29d4867b0b15431a14e40dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f10b64c1995c400ab262aa2c91e81a6b",
              "IPY_MODEL_5e5eff9adc674cf08d48fb03d59614d6"
            ]
          }
        },
        "9d836320b29d4867b0b15431a14e40dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f10b64c1995c400ab262aa2c91e81a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_384d6a790d814c55970f2aecfd8f1045",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12577cc27c1e4d9aacbabc99a12df102"
          }
        },
        "5e5eff9adc674cf08d48fb03d59614d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cba8386836c4dc6a9dd689e6081f438",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456/456 [00:00&lt;00:00, 2.08kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_528585b3e5c6403b940446cb80bf39a9"
          }
        },
        "384d6a790d814c55970f2aecfd8f1045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12577cc27c1e4d9aacbabc99a12df102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cba8386836c4dc6a9dd689e6081f438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "528585b3e5c6403b940446cb80bf39a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "672d461300004701b3de25fdfc3ff0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dc1d64bbe2b94260afd4b95618af5d02",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7b941ef6e534ae3b2f515935d9c4ced",
              "IPY_MODEL_fee6e455a20441d88a5335b24946b94e"
            ]
          }
        },
        "dc1d64bbe2b94260afd4b95618af5d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7b941ef6e534ae3b2f515935d9c4ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd901c6d2d234ab28fe1b654b5697491",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242120,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242120,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4904626c49b14bc5be8f2b59fb844367"
          }
        },
        "fee6e455a20441d88a5335b24946b94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0928d105a06a41fd8ebf764ae766dca8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242k/242k [00:00&lt;00:00, 371kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fa10db985284f34b0555e123a87ae5e"
          }
        },
        "cd901c6d2d234ab28fe1b654b5697491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4904626c49b14bc5be8f2b59fb844367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0928d105a06a41fd8ebf764ae766dca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fa10db985284f34b0555e123a87ae5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f684a8c99ca24978aee64a54b40282ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1903bf44190e4fdab8b0a38c4c014378",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3331dae0b2924eb28bb79943e7fbe4ba",
              "IPY_MODEL_f6244a6272a749ad9fdfeb8ee53b2b2f"
            ]
          }
        },
        "1903bf44190e4fdab8b0a38c4c014378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3331dae0b2924eb28bb79943e7fbe4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_56ca35f5baba453a9857e4b7225708e5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a47a999c2cea41759f57e029ba821acf"
          }
        },
        "f6244a6272a749ad9fdfeb8ee53b2b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57f8503c079347798b18119aebdf36c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 4.98B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2868fb361b2440268105587ebbd965d7"
          }
        },
        "56ca35f5baba453a9857e4b7225708e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a47a999c2cea41759f57e029ba821acf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57f8503c079347798b18119aebdf36c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2868fb361b2440268105587ebbd965d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "027e4915b1d2422299654d7471270949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48d824dbe9244d9192304fcddaca69db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb677a48f5a9487880556d9c86b8f3e4",
              "IPY_MODEL_40ec80ccc5664f2191955648d0de52b9"
            ]
          }
        },
        "48d824dbe9244d9192304fcddaca69db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb677a48f5a9487880556d9c86b8f3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3b76f214bca4496b87b870e19e60e9d2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08ee68c20bbc4ac0a8ae0ce143844e70"
          }
        },
        "40ec80ccc5664f2191955648d0de52b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_682a2de2f30e472883d53eb07ca4ac3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 557B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e29e277d08164fdcb1e73bf6808a550e"
          }
        },
        "3b76f214bca4496b87b870e19e60e9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08ee68c20bbc4ac0a8ae0ce143844e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "682a2de2f30e472883d53eb07ca4ac3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e29e277d08164fdcb1e73bf6808a550e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9adb243a21744be28398d96baaadd996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_437e871cebca4ad38f75c33bfae40e84",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_91dce1fc5aa34faea58372103990fce6",
              "IPY_MODEL_40f1dc4dca1d439c80e462b64adadce1"
            ]
          }
        },
        "437e871cebca4ad38f75c33bfae40e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91dce1fc5aa34faea58372103990fce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7897b1a131f541fb8c9b266a825c6f2b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 43,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 43,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e00ef6987344fb3866434e954aa6a17"
          }
        },
        "40f1dc4dca1d439c80e462b64adadce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b606cfd39967480b94bdf94a6900c1f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 43.0/43.0 [13:26&lt;00:00, 18.7s/B]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c81850779ac4dd59ffa7dc502b2561f"
          }
        },
        "7897b1a131f541fb8c9b266a825c6f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e00ef6987344fb3866434e954aa6a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b606cfd39967480b94bdf94a6900c1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c81850779ac4dd59ffa7dc502b2561f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a8673444ec04bfcbacea1745f973a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c1132ddc9bc4752b1686a578210d02a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_301a84f7c7e34af991520b5f47148cc7",
              "IPY_MODEL_e397706b0bac454dadc9db7830919dd3"
            ]
          }
        },
        "1c1132ddc9bc4752b1686a578210d02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "301a84f7c7e34af991520b5f47148cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_425a4f24dd584fa499a9c59126e5a939",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 441944381,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 441944381,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a49e3a354e484038a01be4de34a4d297"
          }
        },
        "e397706b0bac454dadc9db7830919dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a3b16c553624a65a156e0f914c22dd4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442M/442M [00:12&lt;00:00, 35.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1dd8e5052884401a4a05a3442771dee"
          }
        },
        "425a4f24dd584fa499a9c59126e5a939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a49e3a354e484038a01be4de34a4d297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a3b16c553624a65a156e0f914c22dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1dd8e5052884401a4a05a3442771dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzpUaA_Ja58k"
      },
      "source": [
        "# *Se* multiclass classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUHp4o9Oa58m"
      },
      "source": [
        "01/11/2020\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-26l_Rba58n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5U5F04va58o"
      },
      "source": [
        "This notebook is part of a set of research experiments on Spanish *se* constructions. This notebook contains the code for training and testing *se* multiclass classification models. The data to do so is a gold standard corpus composed of 2,140 sentences (1,713 training sentences; 427 testing sentences) containing the word *se*. The multiclass classification task proposed in this notebook has to do with assigning a single defining tag to every instance of *se* that appears in the cotpus, that is, predicting the specific and exclusing properties of *se* in each sentence. The tag set used to annotate the corpus is composed of four tags: *expl*, *se-mark*, *iobj* and *obj*. The notebook is structured as follows:\n",
        "    \n",
        "1.   Preliminaries    \n",
        "    1.1. Data loading    \n",
        "    1.2. Data preparation\n",
        "\n",
        "2.   Modelling and evaluation    \n",
        "    2.1.   Baseline models    \n",
        "    2.2.   Bag of words models     \n",
        "    2.3.   HashingVectorizer models    \n",
        "    2.4.   TF-IDF models    \n",
        "    2.5.   BETO models    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDb4Znbwa58p"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLD4gsGga58q"
      },
      "source": [
        "## 1. Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzR4dfJvbs9G"
      },
      "source": [
        "### 1.1. Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QRqxQ5ba58r"
      },
      "source": [
        "Load train and test data from a `data` folder. Train and test partitions are set apart beforehand to use the same test dataset in case the train dataset grows. Besides, create a `preds` folder to save the predicitions the models generate. Since the tag distribution in the gold standard corpus is very unbalanced, four different scenarios strategies are tested to raise the number of correct cases of the less frequent categories: \n",
        "1.   Benchmark: using train and test datasets.    \n",
        "2.   Using `f1-macro` parameter over GridSearch.    \n",
        "3.   Using `class_weighted='balanced'` parameter over LinearSVC.     \n",
        "4.   Using an oversampled version of the train dataset (`se_classification_balanced_train`).    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iysYADkUa58t"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('./se_classification_train.csv', delimiter='\\t', index_col='id')\n",
        "train_oversampling = pd.read_csv('./se_classification_balanced_train.csv', delimiter='\\t', index_col='id')\n",
        "test = pd.read_csv('./se_classification_test.csv', delimiter='\\t', index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVED2iJe2UH"
      },
      "source": [
        "Check the content in saved in the variables `train` and `test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjaUyIxa583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "d333413f-cfe9-48e7-b471-5f3cc3449003"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>se_tag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Finalmente, el aragonÃ©s se hizo con su tercera...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Les recomienda que visiten la web (www.farmace...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AhÃ­ se desfondÃ³ el Deportivo, igual que Guarda...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dio un golpe de timÃ³n para adjudicarse la prim...</td>\n",
              "      <td>iobj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SÃ³lo se dirigÃ­a a mÃ­ para pedirme cosas, que s...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text se_tag\n",
              "id                                                          \n",
              "1   Finalmente, el aragonÃ©s se hizo con su tercera...   expl\n",
              "2   Les recomienda que visiten la web (www.farmace...   expl\n",
              "3   AhÃ­ se desfondÃ³ el Deportivo, igual que Guarda...   expl\n",
              "4   Dio un golpe de timÃ³n para adjudicarse la prim...   iobj\n",
              "5   SÃ³lo se dirigÃ­a a mÃ­ para pedirme cosas, que s...   expl"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx0z6ehz037l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "b38e2d5b-69fd-4945-abe0-4c556053b769"
      },
      "source": [
        "train_oversampling.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>se_tag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Finalmente, el aragonÃ©s se hizo con su tercera...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Les recomienda que visiten la web (www.farmace...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AhÃ­ se desfondÃ³ el Deportivo, igual que Guarda...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dio un golpe de timÃ³n para adjudicarse la prim...</td>\n",
              "      <td>iobj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SÃ³lo se dirigÃ­a a mÃ­ para pedirme cosas, que s...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text se_tag\n",
              "id                                                          \n",
              "1   Finalmente, el aragonÃ©s se hizo con su tercera...   expl\n",
              "2   Les recomienda que visiten la web (www.farmace...   expl\n",
              "3   AhÃ­ se desfondÃ³ el Deportivo, igual que Guarda...   expl\n",
              "4   Dio un golpe de timÃ³n para adjudicarse la prim...   iobj\n",
              "5   SÃ³lo se dirigÃ­a a mÃ­ para pedirme cosas, que s...   expl"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tyx0ZZEa59C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "69283254-d2be-45e7-9a68-8405280f4503"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>se_tag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>Las calabazas son el elemento decorativo con e...</td>\n",
              "      <td>se-mark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>SegÃºn el alcalde de la localidad, JosÃ© Dorado ...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>En diez dÃ­as empezarÃ¡ en Barcelona el juego me...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>Sin inmutarse considerÃ³ importante ver los ext...</td>\n",
              "      <td>expl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>Llega con desventaja a la segunda fase, donde ...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text   se_tag\n",
              "id                                                              \n",
              "1713  Las calabazas son el elemento decorativo con e...  se-mark\n",
              "1714  SegÃºn el alcalde de la localidad, JosÃ© Dorado ...     expl\n",
              "1715  En diez dÃ­as empezarÃ¡ en Barcelona el juego me...     expl\n",
              "1716  Sin inmutarse considerÃ³ importante ver los ext...     expl\n",
              "1717  Llega con desventaja a la segunda fase, donde ...      obj"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPG7EIOhpj-"
      },
      "source": [
        "### 1.2. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvU6RdSFa59J"
      },
      "source": [
        "Divide the train and test datasets into:    \n",
        "*   X takes the variable *text* \n",
        "*   Y takes the variable *se_tag*\n",
        "\n",
        "Besides, assign a number (0 to 3) to each of the four values the *se_tag* field might adquire through the LabelEncoder.\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3pPssGGg0B3"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X_train = train['text'].values\n",
        "X_train_oversampling = train_oversampling['text'].values\n",
        "X_test = test['text'].values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "Y_train = label_encoder.fit_transform(train['se_tag'].values)\n",
        "Y_train_oversampling = label_encoder.fit_transform(train_oversampling['se_tag'].values)\n",
        "Y_test = label_encoder.transform(test['se_tag'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC6BkiYVheVS"
      },
      "source": [
        "Check whether the number of tags is right.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdRLvamQc-xW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b0d6ed-a8a9-4e13-e689-8e533248734a"
      },
      "source": [
        "set(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hYzoZwL1dIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f46ec44-5b82-4f8a-b7a1-6e4c886fd127"
      },
      "source": [
        "set(Y_train_oversampling)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slKuN4dZVMYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f569efac-abfc-4160-8ee7-3feb244b3cb1"
      },
      "source": [
        "set(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-06UjNAtppB"
      },
      "source": [
        "### 1.3 Additional packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZyNTddctySv"
      },
      "source": [
        "The following packages are required to run and optimize this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZztmbWNtpIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fe8410-0790-4987-8429-a51669d276e8"
      },
      "source": [
        "pip install transformers==3.5.1 scikit-optimize==0.8.1 spacy==3.0.* fasttext==0.9.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 17.6MB/s \n",
            "\u001b[?25hCollecting scikit-optimize==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 15.0MB/s \n",
            "\u001b[?25hCollecting spacy==3.0.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/5d/20f8252a9dfe7057721136d83cecb1ca1e0936b21fd7a0a4889d1d6650a8/spacy-3.0.1-cp36-cp36m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.8MB 226kB/s \n",
            "\u001b[?25hCollecting fasttext==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.8.1) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.8.1) (1.4.1)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/ea/fae9f69b6e56407961318e8c73e203097a97c7bd71b30bf1b4f5eb448f28/pydantic-1.7.3-cp36-cp36m-manylinux2014_x86_64.whl (9.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.2MB 60.7MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/b7/2a8da28c6b4db5137d6ba949f0160d4628582b1fef5665ead8bb96ac9346/srsly-2.4.0-cp36-cp36m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (53.0.0)\n",
            "Collecting pathy\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/e0/42a9c6fc14882733d5eb367c29873c41a824a51c467395660a13a940b043/pathy-0.3.5-py3-none-any.whl\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d5/6c58fc97f3098775e46d8202bf248752e626a8096a0ae9d76aa7c485a09c/spacy_legacy-3.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==3.0.*) (1.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/c5/214cc68f99c33aa7071d20398dbdf44da92077a86bed43cddd391d4efe26/thinc-8.0.1-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize==0.8.1) (3.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==3.0.*) (3.4.0)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy==3.0.*) (1.1.1)\n",
            "Collecting contextvars<3,>=2.4; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 13.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fasttext, sacremoses, smart-open, contextvars\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3092045 sha256=d1f3a85e47be4729e01b37429953391475688659800454a9f092419a7d550a60\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=258871460878a13b68fd9e544d3f4fa113b006071c94059cb2eea87b0846b520\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp36-none-any.whl size=107097 sha256=adbd52e1e6c57022aad26ef3a30593fe1672ffe23b5ce388fcfaff5df645dff6\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7667 sha256=c078469eba902bf75c602b8ed295ac62f4906260b86a219d1de0a7b789af5392\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built fasttext sacremoses smart-open contextvars\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, pyaml, scikit-optimize, typer, pydantic, catalogue, srsly, smart-open, pathy, spacy-legacy, immutables, contextvars, thinc, spacy, fasttext\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: smart-open 4.1.2\n",
            "    Uninstalling smart-open-4.1.2:\n",
            "      Successfully uninstalled smart-open-4.1.2\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 contextvars-2.4 fasttext-0.9.2 immutables-0.14 pathy-0.3.5 pyaml-20.4.0 pydantic-1.7.3 sacremoses-0.0.43 scikit-optimize-0.8.1 sentencepiece-0.1.91 smart-open-3.0.0 spacy-3.0.1 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 tokenizers-0.9.3 transformers-3.5.1 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzpostHua-Km"
      },
      "source": [
        "### 1.4 General imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tia5LvxbDBh"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zR67w1qa59S"
      },
      "source": [
        "## 2. Modelling and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6xLe7_3a59S"
      },
      "source": [
        "In this section, eight different models are trained with different configurations to maximize the number of correct answers. After the training procedure, models are avaluated using *macro avg F-score*. Some of the parameters that govern the training procedure are defined here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILKg0_SNfRaw"
      },
      "source": [
        "TUNING_ITERATIONS = 30\n",
        "VECTORIZER_BINARY = [True, False]\n",
        "VECTORIZER_N_GRAM = [(1,1), (1,2), (1,3), (1,4), (1,5), (2,2), (3,3), (3,5), (5,5), (5,7), (7,7), (7,9), (10,10)]\n",
        "RF_ESTIMATORS = [10, 100, 1000]\n",
        "RF_MAX_DEPTH = [3, 5, 10, 15, 20, 25, 30, None]\n",
        "SVC_C = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQmsuKIMa59T"
      },
      "source": [
        "### 2.1. Base line models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpzqE0w1B2CS"
      },
      "source": [
        "#### 2.1.1 Base line model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmKBhirma59U"
      },
      "source": [
        "This model asigns the most frequent tag (se-mark, tag 3) to the whole test set and checks the number of correct answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTsv4ystsb2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3cb24e-6b2a-4918-c027-e061e3cb7951"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "baseline_preds = np.full(Y_test.shape, 3)\n",
        "np.save('preds/baseline_preds', baseline_preds)\n",
        "baseline_preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD0v_O5pa59f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a7c5eb-f0af-4803-d519-2bcd42d69782"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, baseline_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.49      1.00      0.66       210\n",
            "\n",
            "    accuracy                           0.49       428\n",
            "   macro avg       0.12      0.25      0.16       428\n",
            "weighted avg       0.24      0.49      0.32       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVDEULhUa59q"
      },
      "source": [
        "### 2.2. Bag of Words models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vnqaH1a5-O"
      },
      "source": [
        "#### 2.2.1. Non-linear CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKSxSjvz8ljv"
      },
      "source": [
        "##### 2.2.1.1. Non-linear CountVectorizer (benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRizWfsba5-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15600fd-7c31-42b5-d7d5-66c2887bef7f"
      },
      "source": [
        "cvrfgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "cvrfgs_model = RandomizedSearchCV(cvrfgs_model, params, n_iter=TUNING_ITERATIONS, n_jobs = -1, cv=StratifiedKFold(), random_state=12345, verbose=2)\n",
        "\n",
        "cvrfgs_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvrfgs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (3, 5), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0izlUrFa5-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55207286-fe04-4134-af4a-22f5590fc394"
      },
      "source": [
        "cvrfgs_preds = cvrfgs_model.predict(X_test)\n",
        "np.save('preds/cvrfgs_preds', cvrfgs_preds)\n",
        "print(classification_report(Y_test, cvrfgs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.73      0.63       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.68      0.65      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.56      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UHx46KO7kJ5"
      },
      "source": [
        "##### 2.2.1.2. Non-linear CountVectorizer (`f1-macro`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWew6k4U5SHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fda5d4-6288-45c8-a468-d05f159eb922"
      },
      "source": [
        "cvrfgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "cvrfgs_f1_macro_model = RandomizedSearchCV(cvrfgs_model, params, scoring='f1_macro', n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvrfgs_f1_macro_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvrfgs_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   37.5s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (3, 5), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zstzSC4p7zhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547c0e65-0430-47f0-f66c-764873d020bd"
      },
      "source": [
        "cvrfgs_f1_macro_preds = cvrfgs_f1_macro_model.predict(X_test)\n",
        "np.save('preds/cvrfgs_f1_macro_preds', cvrfgs_f1_macro_preds)\n",
        "print(classification_report(Y_test, cvrfgs_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.73      0.63       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.68      0.65      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.56      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3m66bzmiCt"
      },
      "source": [
        "##### 2.2.1.3. Non-linear CountVectorizer (`class_weight='balanced'`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yURqoXNBmsB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5940287-a3b6-462b-f0c2-0df75e90afe9"
      },
      "source": [
        "cvrfgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "cvrfgs_class_weight_model = RandomizedSearchCV(cvrfgs_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvrfgs_class_weight_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvrfgs_class_weight_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   36.5s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  4.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSxKmgSXm3zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab5792e-9535-4b0b-9b16-114ce17bbeaf"
      },
      "source": [
        "cvrfgs_class_weight_model_preds = cvrfgs_class_weight_model.predict(X_test)\n",
        "np.save('preds/cvrfgs_class_weight_model_preds', cvrfgs_class_weight_model_preds)\n",
        "print(classification_report(Y_test, cvrfgs_class_weight_model_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.65      0.61       173\n",
            "           1       1.00      0.03      0.06        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.74      0.70       210\n",
            "\n",
            "    accuracy                           0.63       428\n",
            "   macro avg       0.56      0.35      0.34       428\n",
            "weighted avg       0.63      0.63      0.59       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baj7NgQs8VrD"
      },
      "source": [
        "##### 2.2.1.4. Non-linear CountVectorizer (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_pPSsCl8heB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f396e94-50b4-484f-f741-f5df4ca19b30"
      },
      "source": [
        "cvrfgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "cvrfgs_oversampling_model = RandomizedSearchCV(cvrfgs_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvrfgs_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "\n",
        "print(cvrfgs_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   54.7s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R__CboDC8hQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b225ad-f29c-4d51-c097-8287fe40f11d"
      },
      "source": [
        "cvrfgs_oversampling_preds = cvrfgs_oversampling_model.predict(X_test)\n",
        "np.save('preds/cvrfgs_oversampling_preds', cvrfgs_oversampling_preds)\n",
        "print(classification_report(Y_test, cvrfgs_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59       173\n",
            "           1       0.50      0.03      0.06        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.65      0.75      0.70       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.43      0.35      0.34       428\n",
            "weighted avg       0.59      0.62      0.59       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ4Z2P4MlaNS"
      },
      "source": [
        "#### 2.2.2. Linear CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKFVvHaB7wT"
      },
      "source": [
        "##### 2.2.2.1. Linear CountVectorizer (benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNtnuZQtlVFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20eee42-0441-4f0e-891b-9fde42f5b175"
      },
      "source": [
        "cvovrgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "cvovrgs_model = RandomizedSearchCV(cvovrgs_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvovrgs_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvovrgs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUj6uVvJlUYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b0bb46-880a-4d01-bb6e-f65a64241544"
      },
      "source": [
        "cvovrgs_preds = cvovrgs_model.predict(X_test)\n",
        "np.save('preds/cvovrgs_preds', cvovrgs_preds)\n",
        "print(classification_report(Y_test, cvovrgs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.65      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.65      0.71      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mie3pNilCQuG"
      },
      "source": [
        "##### 2.2.2.2. Linear CountVectorizer (`f1_macro`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx9ASRGgCRfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8459e950-2257-42aa-f43c-e5f7b6e5b01c"
      },
      "source": [
        "cvovrgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "cvovrgs_f1_macro_model = RandomizedSearchCV(cvovrgs_model, params, scoring='f1_macro', n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvovrgs_f1_macro_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvovrgs_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIenYaa_CP4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac67c305-f6af-4c43-bc20-836dfe991044"
      },
      "source": [
        "cvovrgs_f1_macro_preds = cvovrgs_f1_macro_model.predict(X_test)\n",
        "np.save('preds/cvovrgs_f1_macro_preds', cvovrgs_f1_macro_preds)\n",
        "print(classification_report(Y_test, cvovrgs_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.65      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.65      0.71      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfprlLy9H52M"
      },
      "source": [
        "##### 2.2.2.3. Linear CountVectorizer( `class_weight='balanced'`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL49uIsqIjIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fd1fef-5381-493c-a7a4-7370d4c6d575"
      },
      "source": [
        "cvovrgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "cvovrgs_class_weight_model = RandomizedSearchCV(cvovrgs_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvovrgs_class_weight_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvovrgs_class_weight_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68uvqKL3Ii6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73a9199-825a-4e03-a813-b97b0d7ea616"
      },
      "source": [
        "cvovrgs_class_weight_preds = cvovrgs_class_weight_model.predict(X_test)\n",
        "np.save('preds/cvovrgs_class_weight_preds', cvovrgs_class_weight_preds)\n",
        "print(classification_report(Y_test, cvovrgs_class_weight_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.64      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.64      0.71      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.30      0.34      0.32       428\n",
            "weighted avg       0.54      0.61      0.57       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icXqrBC4J3ZY"
      },
      "source": [
        "##### 2.2.2.4. Linear CountVectorizer(`f1_macro`, `class_weight='balanced'`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK42ggDsKVi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc581f4-070f-45f8-bc39-1210381ac845"
      },
      "source": [
        "cvovrgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "cvovrgs_class_f1_model = RandomizedSearchCV(cvovrgs_model, params, scoring='f1_macro', n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvovrgs_class_f1_model.fit(X_train, Y_train)\n",
        "\n",
        "print(cvovrgs_class_f1_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54NduBHXKkdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b20bb71-5820-4871-efc8-5b657614eabe"
      },
      "source": [
        "cvovrgs_class_f1_preds = cvovrgs_class_f1_model.predict(X_test)\n",
        "np.save('preds/cvovrgs_class_f1_preds', cvovrgs_class_f1_preds)\n",
        "print(classification_report(Y_test, cvovrgs_class_f1_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.64      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.64      0.71      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.30      0.34      0.32       428\n",
            "weighted avg       0.54      0.61      0.57       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCnvsQiBK5u-"
      },
      "source": [
        "##### 2.2.2.5. Linear CountVectorizer (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJcHYsALCto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e99b87-8006-401d-df21-86802032db6c"
      },
      "source": [
        "cvrfgs_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "cvrfgs_oversampling_model = RandomizedSearchCV(cvrfgs_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "cvrfgs_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "\n",
        "print(cvrfgs_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 23.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yl0g4IRLCS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2962822-fd24-4b59-d6a5-b0c996c6738c"
      },
      "source": [
        "cvrfgs_oversampling_preds = cvrfgs_oversampling_model.predict(X_test)\n",
        "np.save('preds/cvrfgs_oversampling_preds', cvrfgs_oversampling_preds)\n",
        "print(classification_report(Y_test, cvrfgs_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.68      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.66      0.66       210\n",
            "\n",
            "    accuracy                           0.60       428\n",
            "   macro avg       0.30      0.33      0.32       428\n",
            "weighted avg       0.54      0.60      0.57       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qavDIa-ba5-c"
      },
      "source": [
        "### 2.3. HashingVectorizer models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keo0c4Ypa5-0"
      },
      "source": [
        "#### 2.3.1. Non-linear HashingVectorizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F_OuOA4W4--"
      },
      "source": [
        "##### 2.3.1.1. Non-linear HashingVectorizer (benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-sjKnnCa5_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255393e0-a69f-4cbc-e2db-e6a5ed4a46bd"
      },
      "source": [
        "hvrf_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "hvrfgs_model = RandomizedSearchCV(hvrf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvrfgs_model.fit(X_train, Y_train)\n",
        "print(hvrfgs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 42.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiPsso7a5_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f676f53-dd77-4010-c542-76a917553d35"
      },
      "source": [
        "hvrfgs_preds = hvrfgs_model.predict(X_test)\n",
        "np.save('preds/hvrfgs_preds', hvrfgs_preds)\n",
        "print(classification_report(Y_test, hvrfgs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.66      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.69      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.30      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.57       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqV0b_1Xuam"
      },
      "source": [
        "##### 2.3.1.2. Non-linear HashingVectorizer (`f1_macro`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njZL7F6xX2uT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4886d0ce-0013-4a0a-cebd-195978b64707"
      },
      "source": [
        "hvrf_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "hvrfgs_f1_macro_model = RandomizedSearchCV(hvrf_model, params, n_jobs = -1, scoring='f1_macro', cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvrfgs_f1_macro_model.fit(X_train, Y_train)\n",
        "print(hvrfgs_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 43.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo8GWfHkX2Zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce32060-a6ba-401e-dcf8-33ad4ebb06d4"
      },
      "source": [
        "hvrfgs_f1_macro_preds = hvrfgs_f1_macro_model.predict(X_test)\n",
        "np.save('preds/hvrfgs_f1_macro_preds', hvrfgs_f1_macro_preds)\n",
        "print(classification_report(Y_test, hvrfgs_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.66      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.69      0.67       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.30      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.57       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYlxU1gZnbi3"
      },
      "source": [
        "##### 2.3.1.2. Non-linear HashingVectorizer (`class_weight=balanced`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bDyjJcpnjLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d4f141-7aca-43fa-bfbc-8b0e1a3367ec"
      },
      "source": [
        "hvrf_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1, class_weight='balanced'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "hvrfgs_class_weight_model = RandomizedSearchCV(hvrf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvrfgs_class_weight_model.fit(X_train, Y_train)\n",
        "print(hvrfgs_class_weight_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 53.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (3, 5), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPMtlyy8njA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8da8e9-c015-4b30-ef4b-d48705d66b3f"
      },
      "source": [
        "hvrfgs_class_weight_model_preds = hvrfgs_class_weight_model.predict(X_test)\n",
        "np.save('preds/hvrfgs_f1_macro_preds', hvrfgs_class_weight_model_preds)\n",
        "print(classification_report(Y_test, hvrfgs_class_weight_model_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.63      0.58       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.63      0.68      0.65       210\n",
            "\n",
            "    accuracy                           0.59       428\n",
            "   macro avg       0.29      0.33      0.31       428\n",
            "weighted avg       0.53      0.59      0.55       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqtru2f7X8SM"
      },
      "source": [
        "##### 2.3.1.3. Non-linear HashingVectorizer (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DdKsMftYFFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5455396e-4bfc-4ead-9abb-7da3b97f8f4b"
      },
      "source": [
        "hvrf_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "    'classifier__max_depth' : RF_MAX_DEPTH,\n",
        "}\n",
        "\n",
        "hvrfgs_oversampling_model = RandomizedSearchCV(hvrf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvrfgs_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "print(hvrfgs_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 62.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000, 'classifier__max_depth': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PTLtVaWYC45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c747cb-692b-4da8-c593-1823ad9b31ff"
      },
      "source": [
        "hvrfgs_oversampling_preds = hvrfgs_oversampling_model.predict(X_test)\n",
        "np.save('preds/hvrfgs_oversampling_preds', hvrfgs_oversampling_preds)\n",
        "print(classification_report(Y_test, hvrfgs_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.60      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.64      0.78      0.70       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.62      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2NDF58Na6AF"
      },
      "source": [
        "#### 2.3.2. Linear HashingVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMY7nONYSH8"
      },
      "source": [
        "##### 2.3.2.1. Linear HashingVectorizer (benchmark)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeJJohZRa6AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454c3273-7a6d-4839-8935-7ad533f4719b"
      },
      "source": [
        "hvovr_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "hvovr_model = RandomizedSearchCV(hvovr_model, params, n_jobs=-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvovr_model.fit(X_train, Y_train)\n",
        "print(hvovr_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  8.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O3ZAsPHa6AK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c568571-9f16-48e6-b7f6-d5cbfeeded3f"
      },
      "source": [
        "hvovr_preds = hvovr_model.predict(X_test)\n",
        "np.save('preds/hvovr_preds', hvovr_preds)\n",
        "print(classification_report(Y_test, hvovr_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.68      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.65      0.64      0.65       210\n",
            "\n",
            "    accuracy                           0.59       428\n",
            "   macro avg       0.30      0.33      0.31       428\n",
            "weighted avg       0.53      0.59      0.56       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLj5X83XajRZ"
      },
      "source": [
        "##### 2.3.2.2. Linear HashingVectorizer (`f1_macro`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IiS7-aybKIm",
        "colab": {
          "background_save": true
        },
        "outputId": "1716b2b7-a084-4dba-97ef-ec656653ace0"
      },
      "source": [
        "hvovr_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "hvovr_f1_macro_model = RandomizedSearchCV(hvovr_model, params, n_jobs=-1, scoring='f1_macro', cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvovr_f1_macro_model.fit(X_train, Y_train)\n",
        "print(hvovr_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY5gHylgbVbH",
        "colab": {
          "background_save": true
        },
        "outputId": "cd9e6f66-9984-468a-90af-8511fc16e1be"
      },
      "source": [
        "hvovr_f1_macro_preds = hvovr_f1_macro_model.predict(X_test)\n",
        "np.save('preds/hvovr_f1_macro_preds', hvovr_f1_macro_preds)\n",
        "print(classification_report(Y_test, hvovr_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.68      0.60       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.65      0.64      0.65       210\n",
            "\n",
            "    accuracy                           0.59       428\n",
            "   macro avg       0.30      0.33      0.31       428\n",
            "weighted avg       0.53      0.59      0.56       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng3UtnqwauOR"
      },
      "source": [
        "##### 2.3.2.3. Linear HashingVectorizer (`class_weight`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Rx3cYObhiK",
        "colab": {
          "background_save": true
        },
        "outputId": "f6422c19-e17e-4e7d-8728-a148c94042e1"
      },
      "source": [
        "hvovr_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "hvovr_class_weight_model = RandomizedSearchCV(hvovr_model, params, n_jobs=-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvovr_class_weight_model.fit(X_train, Y_train)\n",
        "print(hvovr_class_weight_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  7.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAqVFm42blnc",
        "colab": {
          "background_save": true
        },
        "outputId": "c6e9eca5-a93b-494d-e89c-703ef46391b8"
      },
      "source": [
        "hvovr_class_weight_preds = hvovr_class_weight_model.predict(X_test)\n",
        "np.save('preds/hvovr_class_weight_preds', hvovr_class_weight_preds)\n",
        "print(classification_report(Y_test, hvovr_class_weight_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.70      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidfJFLqa3ej"
      },
      "source": [
        "##### 2.3.2.4. Linear HashingVectorizer (`f1_macro`, `class_weight`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYffGEXEbivQ",
        "colab": {
          "background_save": true
        },
        "outputId": "e5addde6-39e1-4353-b82f-25f6fc7cd73d"
      },
      "source": [
        "hvovr_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "hvovr_class_f1_model = RandomizedSearchCV(hvovr_model, params, n_jobs=-1, scoring='f1_macro', cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvovr_class_f1_model.fit(X_train, Y_train)\n",
        "print(hvovr_class_f1_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  7.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-7_mvUbmja",
        "colab": {
          "background_save": true
        },
        "outputId": "43a7821a-85a1-47aa-aece-73eaa7c5dd4a"
      },
      "source": [
        "hvovr_class_f1_preds = hvovr_class_f1_model.predict(X_test)\n",
        "np.save('preds/hvovr_class_f1_preds', hvovr_class_f1_preds)\n",
        "print(classification_report(Y_test, hvovr_class_f1_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.70      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzLrOoUIbANz"
      },
      "source": [
        "##### 2.3.2.5. Linear HashingVectorizer (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMjUicwAbjvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5bd1c0-6a83-46d9-fe7d-4171c714aed4"
      },
      "source": [
        "hvovr_model = Pipeline([\n",
        "    ('vectorizer', HashingVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "hvovr_oversampling_model = RandomizedSearchCV(hvovr_model, params, n_jobs=-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "hvovr_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "print(hvovr_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 27.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 5), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 10.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_y2apahbnSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65898d19-1f84-4dca-a12b-211c81e8f312"
      },
      "source": [
        "hvovr_oversampling_preds = hvovr_oversampling_model.predict(X_test)\n",
        "np.save('preds/hvovr_oversampling_preds', hvovr_oversampling_preds)\n",
        "print(classification_report(Y_test, hvovr_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.69      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.68      0.68      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_qjCrCa6Bx"
      },
      "source": [
        "### 2.4. TF-IDF models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ptG2F9a6By"
      },
      "source": [
        "#### 2.4.1 Non-linear TFD-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZWBKIHa6Cb"
      },
      "source": [
        "##### 2.4.1.1. Non-linear TFI-DF (benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKIXvIkYa6Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdaf8ac6-4eb0-4521-959c-26be258e6c29"
      },
      "source": [
        "tf_idf_rf_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "}\n",
        "\n",
        "tf_idf_rf_gs_model = RandomizedSearchCV(tf_idf_rf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tf_idf_rf_gs_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tf_idf_rf_gs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 10.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5X6rlhwa6Cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbc0995-67c3-4d64-e2aa-06663950484c"
      },
      "source": [
        "tf_idf_rf_gs_preds = tf_idf_rf_gs_model.predict(X_test)\n",
        "np.save('preds/tf_idf_rf_gs_preds', tf_idf_rf_gs_preds)\n",
        "print(classification_report(Y_test, tf_idf_rf_gs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.67      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.67      0.70      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nIqvnMdeGrP"
      },
      "source": [
        "##### 2.4.1.2. Non-linear TF-IDF (`f1_macro`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NliYM8Ih4nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e05dc1-57b5-4c65-c864-8256d7470408"
      },
      "source": [
        "tfidfrf_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "}\n",
        "\n",
        "tfidfrfgs_f1_macro_model = RandomizedSearchCV(tfidfrf_model, params, scoring='f1_macro', n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfrfgs_f1_macro_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tfidfrfgs_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 10.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyQhMTbfh-BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef89ea2a-4348-4e4c-b9f3-dd5a9c0eaf6f"
      },
      "source": [
        "tfidfrfgs_f1_macro_preds = tfidfrfgs_f1_macro_model.predict(X_test)\n",
        "np.save('preds/tfidfrfgs_f1_macro_preds', tfidfrfgs_f1_macro_preds)\n",
        "print(classification_report(Y_test, tfidfrfgs_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.67      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.67      0.70      0.68       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.55      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xiU-efn8Tz"
      },
      "source": [
        "##### 2.4.1.3. Non-linear TF-IDF (`class_weight='balanced'`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIVUctlTn9iQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c1aac8-c793-498c-af08-a834ad8d64ab"
      },
      "source": [
        "tf_idf_rf_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "}\n",
        "\n",
        "tf_idf_rf_gs_model = RandomizedSearchCV(tf_idf_rf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tf_idf_rf_gs_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tf_idf_rf_gs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 11.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu0tWGrDn9XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6270cbaa-83f0-495f-e88d-808438357988"
      },
      "source": [
        "tf_idf_rf_gs_preds = tf_idf_rf_gs_model.predict(X_test)\n",
        "np.save('preds/tf_idf_rf_gs_preds', tf_idf_rf_gs_preds)\n",
        "print(classification_report(Y_test, tf_idf_rf_gs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.67      0.62       173\n",
            "           1       1.00      0.03      0.06        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.67      0.72      0.70       210\n",
            "\n",
            "    accuracy                           0.63       428\n",
            "   macro avg       0.56      0.36      0.35       428\n",
            "weighted avg       0.64      0.63      0.60       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYyO4hV_eNSp"
      },
      "source": [
        "##### 2.4.1.4. Non-linear TF-IDF (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW0wQhoPigrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e00128-5a8c-4692-d397-796bb1b81a17"
      },
      "source": [
        "tfidfrf_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, n_jobs = -1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__n_estimators' : RF_ESTIMATORS,\n",
        "}\n",
        "\n",
        "tfidfrfgs_oversampling_model = RandomizedSearchCV(tfidfrf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfrfgs_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "\n",
        "print(tfidfrfgs_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 16.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 7), 'vectorizer__binary': True, 'classifier__n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2rCAowoidqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9215e3b-4531-4e63-c7e7-d3f1a7dd352f"
      },
      "source": [
        "tfidfrfgs_oversampling_preds = tfidfrfgs_oversampling_model.predict(X_test)\n",
        "np.save('preds/tfidfrfgs_oversampling_preds', tfidfrfgs_oversampling_preds)\n",
        "print(classification_report(Y_test, tfidfrfgs_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.60      0.59       173\n",
            "           1       0.50      0.03      0.06        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.64      0.77      0.70       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.43      0.35      0.34       428\n",
            "weighted avg       0.59      0.62      0.59       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T52_6RY1a6Cv"
      },
      "source": [
        "#### 2.4.2. Linear TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo3IrWLNeb3R"
      },
      "source": [
        "##### 2.4.2.1. Linear TF-IDF (benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIfYKMp3a6Cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99603bb-437c-4b8a-8d05-0cdc6eddd2dd"
      },
      "source": [
        "tfidfovrsvc_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "tfidfovrsvcgs_model = RandomizedSearchCV(tfidfovrsvc_model, params, n_jobs =-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfovrsvcgs_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tfidfovrsvcgs_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   53.2s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  2.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 5), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_E3zGu1a6Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d28e88-b0e7-4b34-86ab-374e7ac65eb0"
      },
      "source": [
        "tfidfovrsvcgs_preds = tfidfovrsvcgs_model.predict(X_test)\n",
        "np.save('preds/tfidfovrsvcgs_preds', tfidfovrsvcgs_preds)\n",
        "print(classification_report(Y_test, tfidfovrsvcgs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.75      0.65       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.72      0.70      0.71       210\n",
            "\n",
            "    accuracy                           0.64       428\n",
            "   macro avg       0.32      0.36      0.34       428\n",
            "weighted avg       0.58      0.64      0.61       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx88wKSyetb7"
      },
      "source": [
        "##### 2.4.2.2. Linear TF-IDF (`f1_macro`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39tFVEo6jj3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307349ca-9260-417d-c8e4-e6c9bcd0b945"
      },
      "source": [
        "tfidfovrsvc_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "tfidfovrsvcgs_f1_macro_model = RandomizedSearchCV(tfidfovrsvc_model, params, scoring='f1_macro', n_jobs =-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfovrsvcgs_f1_macro_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tfidfovrsvcgs_f1_macro_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   52.5s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  2.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 5), 'vectorizer__binary': True, 'classifier__estimator__base_estimator__C': 0.001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR6Qb5PNjmBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200e0bb8-71e0-457b-f0ac-f89292d5fa8a"
      },
      "source": [
        "tfidfovrsvcgs_f1_macro_preds = tfidfovrsvcgs_f1_macro_model.predict(X_test)\n",
        "np.save('preds/tfidfovrsvcgs_f1_macro_preds', tfidfovrsvcgs_f1_macro_preds)\n",
        "print(classification_report(Y_test, tfidfovrsvcgs_f1_macro_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.75      0.65       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.72      0.70      0.71       210\n",
            "\n",
            "    accuracy                           0.64       428\n",
            "   macro avg       0.32      0.36      0.34       428\n",
            "weighted avg       0.58      0.64      0.61       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sLmeHYUe72n"
      },
      "source": [
        "##### 2.4.2.3. Linear TF-IDF (`class_weight`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qq-xsnKj2Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67639ad5-ec3b-4b32-d4de-39e8074ebf15"
      },
      "source": [
        "tfidfovrsvc_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "tfidfovrsvcgs_class_weight_model = RandomizedSearchCV(tfidfovrsvc_model, params, n_jobs =-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfovrsvcgs_class_weight_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tfidfovrsvcgs_class_weight_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   53.5s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  2.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (3, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT_0lnKmj166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3086e95-0e6a-430d-a8ad-777c05710d78"
      },
      "source": [
        "tfidfovrsvcgs_class_weight_preds = tfidfovrsvcgs_class_weight_model.predict(X_test)\n",
        "np.save('preds/tfidfovrsvcgs_class_weight_preds', tfidfovrsvcgs_class_weight_preds)\n",
        "print(classification_report(Y_test, tfidfovrsvcgs_class_weight_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.71      0.62       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.69      0.68      0.69       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.31      0.35      0.33       428\n",
            "weighted avg       0.56      0.62      0.59       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUgDGS62fD_F"
      },
      "source": [
        "##### 2.4.2.4. Linear TF-IDF (`f1_macro`, `class_weight`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0feG1-gkQ26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a063a8f8-a9e0-4c63-9a1c-32aa86fe5395"
      },
      "source": [
        "tfidfovrsvc_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced')), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "tfidfovrsvcgs_f1_class_model = RandomizedSearchCV(tfidfovrsvc_model, params, scoring='f1_macro', n_jobs =-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfovrsvcgs_f1_class_model.fit(X_train, Y_train)\n",
        "\n",
        "print(tfidfovrsvcgs_f1_class_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   53.6s\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  3.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (3, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.0001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geK2T8FjkQmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b9b891-e291-4123-bfe4-647898be3dea"
      },
      "source": [
        "tfidfovrsvcgs_f1_class_preds = tfidfovrsvcgs_f1_class_model.predict(X_test)\n",
        "np.save('preds/tfidfovrsvcgs_f1_class_preds', tfidfovrsvcgs_f1_class_preds)\n",
        "print(classification_report(Y_test, tfidfovrsvcgs_f1_class_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.71      0.62       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.69      0.68      0.69       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.31      0.35      0.33       428\n",
            "weighted avg       0.56      0.62      0.59       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBoZ7XexfLVQ"
      },
      "source": [
        "##### 2.4.2.5. Linear TF-IDF (oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GggCgQ0Wkkcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcea9f4-4c32-444d-f263-a6afa33ef204"
      },
      "source": [
        "tfidfovrsvc_model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(analyzer='char_wb')),\n",
        "    ('classifier', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC()), n_jobs=-1))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'vectorizer__binary' : VECTORIZER_BINARY,\n",
        "    'vectorizer__ngram_range': VECTORIZER_N_GRAM,\n",
        "    'classifier__estimator__base_estimator__C': SVC_C,\n",
        "}\n",
        "\n",
        "tfidfovrsvcgs_oversampling_model = RandomizedSearchCV(tfidfovrsvc_model, params, n_jobs =-1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\n",
        "\n",
        "tfidfovrsvcgs_oversampling_model.fit(X_train_oversampling, Y_train_oversampling)\n",
        "\n",
        "print(tfidfovrsvcgs_oversampling_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 16.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (5, 5), 'vectorizer__binary': False, 'classifier__estimator__base_estimator__C': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVshdLzekkMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072fe626-2214-45a8-fbee-3c951d2aff08"
      },
      "source": [
        "tfidfovrsvcgs_oversampling_preds = tfidfovrsvcgs_oversampling_model.predict(X_test)\n",
        "np.save('preds/tfidfovrsvcgs_oversampling_preds', tfidfovrsvcgs_oversampling_preds)\n",
        "print(classification_report(Y_test, tfidfovrsvcgs_oversampling_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.65      0.61       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.66      0.71      0.69       210\n",
            "\n",
            "    accuracy                           0.61       428\n",
            "   macro avg       0.31      0.34      0.32       428\n",
            "weighted avg       0.56      0.61      0.58       428\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axMqgMr3lZbN"
      },
      "source": [
        "### 2.5. Pre-trained language models (transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7N4p4jJp9Fq"
      },
      "source": [
        "#### 2.5.1. Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D2ePCjlp5ET"
      },
      "source": [
        "Collator function for batching the data for the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuz_UZ1uqHfB"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TextClassificationCollator:\n",
        "    \"\"\"Data collator for a text classification problem\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer):\n",
        "        \"\"\"Initializes the collator with a tokenizer\"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    def encode_texts(self, texts):\n",
        "        \"\"\"Transforms an iterable of texts into a dictionary of model input tensors, stored in the GPU\"\"\"\n",
        "        # Tokenize and encode texts as tensors, with maximum length\n",
        "        tensors = self.tokenizer.batch_encode_plus(texts, padding=\"longest\", return_tensors=\"pt\")\n",
        "        # Move tensors to GPU\n",
        "        for key in tensors:\n",
        "            tensors[key] = tensors[key].to(self.device)\n",
        "        return tensors\n",
        "    \n",
        "    def __call__(self, patterns):\n",
        "        \"\"\"Collate a batch of patterns\n",
        "        \n",
        "        Arguments:\n",
        "            - patterns: iterable of tuples in the form (text, class)\n",
        "            \n",
        "        Output: dictionary of torch tensors ready for model input\n",
        "        \"\"\"\n",
        "        # Check kind of input\n",
        "        if len(patterns) < 1: raise ValueError(f\"At least one pattern is required for training, found {len(patterns)}\")\n",
        "        if not isinstance(patterns[0], (tuple, str)): raise ValueError(f\"Each pattern must be one text, or a tuple with text and label. Found {patterns[0]}\")\n",
        "        targets_provided = len(patterns[0]) == 2\n",
        "        # Split texts and classes from the input list of tuples\n",
        "        if targets_provided:\n",
        "            train_idx, targets = zip(*patterns)\n",
        "        else:\n",
        "            train_idx = patterns\n",
        "        # Encode inputs\n",
        "        input_tensors = self.encode_texts(train_idx)\n",
        "        if targets_provided:\n",
        "          # Transform class labels to a tensor in GPU\n",
        "          Y = torch.tensor(targets).long().to(self.device)\n",
        "        # Return batch as a dictionary wikth all the inputs tensors and the labels\n",
        "        batch = {**input_tensors}\n",
        "        if targets_provided:\n",
        "          batch[\"labels\"] = Y\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQI24GmqqYXH"
      },
      "source": [
        "#### 2.5.2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0BEiZ7iqbYy"
      },
      "source": [
        "Define a scikit-learn compatible class for a Tranformers model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm4XMHYyqgFn"
      },
      "source": [
        "from copy import copy\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "class TransformersClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, pretrained_model='dccuchile/bert-base-spanish-wwm-uncased', \n",
        "                 learning_rate=5e-5, num_train_epochs=1, per_device_train_batch_size=8, per_device_eval_batch_size=128, \n",
        "                 attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1, output_dir=\"./transformers_model\", ):\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.per_device_train_batch_size = per_device_train_batch_size \n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.output_dir = output_dir\n",
        "        self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        # Prepare config\n",
        "        num_labels = len(set(y))\n",
        "        config = AutoConfig.from_pretrained(self.pretrained_model, num_labels=num_labels)\n",
        "        config.attention_probs_dropout_prob = self.attention_probs_dropout_prob\n",
        "        config.hidden_dropout_prob = self.hidden_dropout_prob\n",
        "        # Prepare tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model)\n",
        "        # Build collator\n",
        "        collator = TextClassificationCollator(tokenizer)\n",
        "        # Initialize model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.pretrained_model, config=config)\n",
        "        # Prepare training args\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            per_device_eval_batch_size=self.per_device_eval_batch_size,\n",
        "            disable_tqdm=True,\n",
        "            learning_rate = self.learning_rate,\n",
        "            num_train_epochs = self.num_train_epochs,\n",
        "            per_device_train_batch_size = self.per_device_train_batch_size\n",
        "        )\n",
        "        # Initialize trainer\n",
        "        self._trainer = Trainer(\n",
        "            model=model,\n",
        "            data_collator=collator,\n",
        "            args=training_args,\n",
        "            train_dataset=list(zip(X, y))\n",
        "        )\n",
        "        # Train\n",
        "        self._trainer.train()\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = self._trainer.predict(X)\n",
        "        return np.argmax(preds.predictions, axis=1)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        preds = self.predict(X)\n",
        "        return f1_score(y, preds, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWd4CHh2qiIA"
      },
      "source": [
        "#### 2.5.3. Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2liuilY7qlO0"
      },
      "source": [
        "Train basic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDgt-U48qnIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5d5d08-6799-402f-eed5-fd7863082b04"
      },
      "source": [
        "model = TransformersClassifier(pretrained_model='dccuchile/bert-base-spanish-wwm-uncased', num_train_epochs=4)\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.6036767578125, 'learning_rate': 2.0794392523364487e-05, 'epoch': 2.336448598130841}\n",
            "{'epoch': 4.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEGhePqVqpOx"
      },
      "source": [
        "Basic model metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRxkObIPqrqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643e2720-61b6-42b0-a4ec-2bf56ef73398"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = model.predict(X_test)\n",
        "print(classification_report(Y_test, preds))  \n",
        "# 0.66-0.67 with BETO uncased and 4 epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       173\n",
            "           1       0.69      0.65      0.67        31\n",
            "           2       0.40      0.29      0.33        14\n",
            "           3       0.93      0.87      0.90       210\n",
            "\n",
            "    accuracy                           0.84       428\n",
            "   macro avg       0.70      0.67      0.68       428\n",
            "weighted avg       0.84      0.84      0.84       428\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxL8imjoqt6s"
      },
      "source": [
        "Run hyperparameters optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI3l7UoLqvtE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "49e30e29e23a4dba9a83e5ab0ca2ce9e",
            "9d836320b29d4867b0b15431a14e40dc",
            "f10b64c1995c400ab262aa2c91e81a6b",
            "5e5eff9adc674cf08d48fb03d59614d6",
            "384d6a790d814c55970f2aecfd8f1045",
            "12577cc27c1e4d9aacbabc99a12df102",
            "4cba8386836c4dc6a9dd689e6081f438",
            "528585b3e5c6403b940446cb80bf39a9",
            "672d461300004701b3de25fdfc3ff0b4",
            "dc1d64bbe2b94260afd4b95618af5d02",
            "d7b941ef6e534ae3b2f515935d9c4ced",
            "fee6e455a20441d88a5335b24946b94e",
            "cd901c6d2d234ab28fe1b654b5697491",
            "4904626c49b14bc5be8f2b59fb844367",
            "0928d105a06a41fd8ebf764ae766dca8",
            "2fa10db985284f34b0555e123a87ae5e",
            "f684a8c99ca24978aee64a54b40282ab",
            "1903bf44190e4fdab8b0a38c4c014378",
            "3331dae0b2924eb28bb79943e7fbe4ba",
            "f6244a6272a749ad9fdfeb8ee53b2b2f",
            "56ca35f5baba453a9857e4b7225708e5",
            "a47a999c2cea41759f57e029ba821acf",
            "57f8503c079347798b18119aebdf36c7",
            "2868fb361b2440268105587ebbd965d7",
            "027e4915b1d2422299654d7471270949",
            "48d824dbe9244d9192304fcddaca69db",
            "eb677a48f5a9487880556d9c86b8f3e4",
            "40ec80ccc5664f2191955648d0de52b9",
            "3b76f214bca4496b87b870e19e60e9d2",
            "08ee68c20bbc4ac0a8ae0ce143844e70",
            "682a2de2f30e472883d53eb07ca4ac3d",
            "e29e277d08164fdcb1e73bf6808a550e",
            "9adb243a21744be28398d96baaadd996",
            "437e871cebca4ad38f75c33bfae40e84",
            "91dce1fc5aa34faea58372103990fce6",
            "40f1dc4dca1d439c80e462b64adadce1",
            "7897b1a131f541fb8c9b266a825c6f2b",
            "2e00ef6987344fb3866434e954aa6a17",
            "b606cfd39967480b94bdf94a6900c1f1",
            "9c81850779ac4dd59ffa7dc502b2561f",
            "3a8673444ec04bfcbacea1745f973a94",
            "1c1132ddc9bc4752b1686a578210d02a",
            "301a84f7c7e34af991520b5f47148cc7",
            "e397706b0bac454dadc9db7830919dd3",
            "425a4f24dd584fa499a9c59126e5a939",
            "a49e3a354e484038a01be4de34a4d297",
            "0a3b16c553624a65a156e0f914c22dd4",
            "a1dd8e5052884401a4a05a3442771dee"
          ]
        },
        "outputId": "6fc8d573-b529-4793-aecc-9e6924c669d7"
      },
      "source": [
        "%%time\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space.space import Integer, Real\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "param_grid = {\n",
        "    \"pretrained_model\": ['dccuchile/bert-base-spanish-wwm-uncased', 'dccuchile/bert-base-spanish-wwm-cased'],\n",
        "    \"learning_rate\": Real(1e-6, 1e-4, \"log-uniform\"),\n",
        "    \"num_train_epochs\": Integer(1, 10, \"uniform\"),\n",
        "    \"per_device_train_batch_size\": [4, 8, 16, 32, 64],\n",
        "    \"attention_probs_dropout_prob\": Real(0, 0.9, \"uniform\"),\n",
        "    \"hidden_dropout_prob\": Real(0, 0.9, \"uniform\"),\n",
        "}\n",
        "\n",
        "cv_method = StratifiedKFold()\n",
        "\n",
        "metamodel = BayesSearchCV(model, param_grid, n_iter=30, verbose=3, cv=cv_method, random_state=12345, error_score=0.0)\n",
        "metamodel.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 5.0}\n",
            "[CV]  attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.203, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 5.0}\n",
            "[CV]  attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.208, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 5.0}\n",
            "[CV]  attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.223, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 5.0}\n",
            "[CV]  attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.231, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 5.0}\n",
            "[CV]  attention_probs_dropout_prob=0.36712704740889385, hidden_dropout_prob=0.8900230698958589, learning_rate=1.552111989984059e-05, num_train_epochs=5, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.229, total= 1.6min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49e30e29e23a4dba9a83e5ab0ca2ce9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "672d461300004701b3de25fdfc3ff0b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242120.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f684a8c99ca24978aee64a54b40282ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "027e4915b1d2422299654d7471270949",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9adb243a21744be28398d96baaadd996",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=43.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a8673444ec04bfcbacea1745f973a94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=441944381.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.257, total= 1.1min\n",
            "[CV] attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.313, total=  49.2s\n",
            "[CV] attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.287, total=  49.7s\n",
            "[CV] attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.283, total=  49.3s\n",
            "[CV] attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4626460576204746, hidden_dropout_prob=0.2866560163294204, learning_rate=3.5353403590329405e-06, num_train_epochs=2, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.277, total=  48.8s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.3min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0813143310546875, 'learning_rate': 2.3220342612635823e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.152, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.088072021484375, 'learning_rate': 2.3220342612635823e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0956588134765626, 'learning_rate': 2.3220342612635823e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.089074462890625, 'learning_rate': 2.3220342612635823e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.155, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0919476318359376, 'learning_rate': 2.3220342612635823e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.041508544006946244, hidden_dropout_prob=0.6683973763039132, learning_rate=7.488560492575054e-05, num_train_epochs=6, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.180, total= 2.1min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.11713916015625, 'learning_rate': 9.352646795810219e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.166, total= 3.1min\n",
            "[CV] attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.1min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.1155830078125, 'learning_rate': 9.352646795810219e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.194, total= 3.1min\n",
            "[CV] attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.12138232421875, 'learning_rate': 9.352646795810219e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.222, total= 3.1min\n",
            "[CV] attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.1212049560546875, 'learning_rate': 9.352646795810219e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.211, total= 3.1min\n",
            "[CV] attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.1248878173828125, 'learning_rate': 9.352646795810219e-06, 'epoch': 5.813953488372093}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.22314680177365268, hidden_dropout_prob=0.743192364069995, learning_rate=2.641952051079237e-05, num_train_epochs=9, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.182, total= 3.1min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.4min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.281, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.185, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.8min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.183, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.286, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8685989173436767, hidden_dropout_prob=0.6623935066577347, learning_rate=1.0930694523889127e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.295, total= 1.4min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  6.9min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.163, total= 2.8min\n",
            "[CV] attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.8min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.292, total= 2.8min\n",
            "[CV] attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.260, total= 2.8min\n",
            "[CV] attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.176, total= 2.8min\n",
            "[CV] attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.3691514693845362, hidden_dropout_prob=0.7104066631507853, learning_rate=1.233186483718184e-06, num_train_epochs=9, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.230, total= 2.8min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 14.0min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 3.0}\n",
            "[CV]  attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.413, total= 1.0min\n",
            "[CV] attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 3.0}\n",
            "[CV]  attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.390, total= 1.0min\n",
            "[CV] attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 3.0}\n",
            "[CV]  attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.384, total= 1.0min\n",
            "[CV] attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 3.0}\n",
            "[CV]  attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.401, total= 1.0min\n",
            "[CV] attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 3.0}\n",
            "[CV]  attention_probs_dropout_prob=0.03317383878440064, hidden_dropout_prob=0.3094794046302785, learning_rate=4.604040451225829e-05, num_train_epochs=3, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.408, total= 1.0min\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.203, total=  44.6s\n",
            "[CV] attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.6s remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.244, total=  44.2s\n",
            "[CV] attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.259, total=  44.4s\n",
            "[CV] attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.264, total=  44.3s\n",
            "[CV] attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.8431661252980369, hidden_dropout_prob=0.5125256937648924, learning_rate=2.8419114324804943e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.280, total=  44.8s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.7min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.340, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.357, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.7min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.346, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.353, total= 1.4min\n",
            "[CV] attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.21035384276262642, hidden_dropout_prob=0.3577159636430006, learning_rate=9.098505378842472e-06, num_train_epochs=4, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.359, total= 1.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  6.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.253, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.243, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.248, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.185, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6629296110884636, hidden_dropout_prob=0.4455171842906857, learning_rate=1.653381915174362e-06, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.190, total= 2.5min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.6683850708007812, 'learning_rate': 2.04635118490275e-06, 'epoch': 2.9069767441860463}\n",
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.414, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.6699216918945312, 'learning_rate': 2.04635118490275e-06, 'epoch': 2.9069767441860463}\n",
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.406, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.68366162109375, 'learning_rate': 2.04635118490275e-06, 'epoch': 2.9069767441860463}\n",
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.466, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.699948486328125, 'learning_rate': 2.04635118490275e-06, 'epoch': 2.9069767441860463}\n",
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.419, total= 1.6min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.7024149780273438, 'learning_rate': 2.04635118490275e-06, 'epoch': 2.9069767441860463}\n",
            "{'epoch': 4.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.04345946292316195, learning_rate=7.488774549005809e-06, num_train_epochs=4, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.420, total= 1.8min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.405, total=  45.1s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   45.1s remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.405, total=  44.2s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.418, total=  44.4s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.420, total=  44.7s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 2.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.14780354243941785, learning_rate=3.270187943309578e-05, num_train_epochs=2, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.422, total=  44.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.370, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.364, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.346, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.390, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.25058398844147933, hidden_dropout_prob=0.0, learning_rate=5.893355059857079e-06, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.386, total= 3.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.361, total=  23.7s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.7s remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.367, total=  23.5s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   47.2s remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.381, total=  23.0s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.381, total=  23.1s\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, learning_rate=0.0001, num_train_epochs=1, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.409, total=  23.6s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 7.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.319, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 7.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.264, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.3min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 7.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.271, total= 2.1min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 7.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.176, total= 2.2min\n",
            "[CV] attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 7.0}\n",
            "[CV]  attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.2133450718085503, learning_rate=1e-06, num_train_epochs=7, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.162, total= 2.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9052290649414062, 'learning_rate': 1.7931762417415112e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7354226684570313, 'learning_rate': 1.4466017845050382e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5259566650390625, 'learning_rate': 1.1000273272685654e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.36916845703125, 'learning_rate': 7.534528700320923e-06, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.231684326171875, 'learning_rate': 4.068784127956192e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.184382568359375, 'learning_rate': 6.03039555591463e-07, 'epoch': 8.746355685131196}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.538, total= 5.0min\n",
            "[CV] attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8887540283203125, 'learning_rate': 1.7931762417415112e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.737337158203125, 'learning_rate': 1.4466017845050382e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5693077392578125, 'learning_rate': 1.1000273272685654e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.390347412109375, 'learning_rate': 7.534528700320923e-06, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.24666650390625, 'learning_rate': 4.068784127956192e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.166180419921875, 'learning_rate': 6.03039555591463e-07, 'epoch': 8.746355685131196}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.591, total= 5.0min\n",
            "[CV] attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.87962353515625, 'learning_rate': 1.7931762417415112e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.661344482421875, 'learning_rate': 1.4466017845050382e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4279091796875, 'learning_rate': 1.1000273272685654e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.3235322265625, 'learning_rate': 7.534528700320923e-06, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.20436328125, 'learning_rate': 4.068784127956192e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.13380517578125, 'learning_rate': 6.03039555591463e-07, 'epoch': 8.746355685131196}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.642, total= 5.1min\n",
            "[CV] attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8928929443359375, 'learning_rate': 1.7931762417415112e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7714669189453125, 'learning_rate': 1.4466017845050382e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5113388671875, 'learning_rate': 1.1000273272685654e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.360656005859375, 'learning_rate': 7.534528700320923e-06, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.2364052734375, 'learning_rate': 4.068784127956192e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.169201416015625, 'learning_rate': 6.03039555591463e-07, 'epoch': 8.746355685131196}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.665, total= 5.1min\n",
            "[CV] attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8764473876953125, 'learning_rate': 1.7931762417415112e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.75494970703125, 'learning_rate': 1.4466017845050382e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4814168701171875, 'learning_rate': 1.1000273272685654e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.287920654296875, 'learning_rate': 7.534528700320923e-06, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.1749130859375, 'learning_rate': 4.068784127956192e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.142530517578125, 'learning_rate': 6.03039555591463e-07, 'epoch': 8.746355685131196}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5811198071610473, hidden_dropout_prob=0.04922281119176865, learning_rate=2.139750698977984e-05, num_train_epochs=9, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.643, total= 5.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 25.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8865775146484375, 'learning_rate': 2.8045437369074426e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7164647216796876, 'learning_rate': 2.3259526555239202e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5023603515625, 'learning_rate': 1.8473615741403974e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.28520458984375, 'learning_rate': 1.3687704927568749e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.1622255859375, 'learning_rate': 8.901794113733521e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.08736083984375, 'learning_rate': 4.115883299898295e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.624, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.922276123046875, 'learning_rate': 2.8045437369074426e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7674910888671875, 'learning_rate': 2.3259526555239202e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.6139119873046875, 'learning_rate': 1.8473615741403974e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.4315439453125, 'learning_rate': 1.3687704927568749e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.28943505859375, 'learning_rate': 8.901794113733521e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.1570849609375, 'learning_rate': 4.115883299898295e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.655, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 11.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9309478149414062, 'learning_rate': 2.8045437369074426e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7310270385742188, 'learning_rate': 2.3259526555239202e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4825712890625, 'learning_rate': 1.8473615741403974e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.34726953125, 'learning_rate': 1.3687704927568749e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.22194482421875, 'learning_rate': 8.901794113733521e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.144838134765625, 'learning_rate': 4.115883299898295e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.651, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.904473876953125, 'learning_rate': 2.8045437369074426e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.789681396484375, 'learning_rate': 2.3259526555239202e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.52916943359375, 'learning_rate': 1.8473615741403974e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.380943115234375, 'learning_rate': 1.3687704927568749e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.185896484375, 'learning_rate': 8.901794113733521e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.1382666015625, 'learning_rate': 4.115883299898295e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.614, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9049329833984375, 'learning_rate': 2.8045437369074426e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.80059033203125, 'learning_rate': 2.3259526555239202e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.6377781982421875, 'learning_rate': 1.8473615741403974e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.40250634765625, 'learning_rate': 1.3687704927568749e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.286391357421875, 'learning_rate': 8.901794113733521e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.17134375, 'learning_rate': 4.115883299898295e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6057180619823983, hidden_dropout_prob=0.04161782752970779, learning_rate=3.283134818290965e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.619, total= 5.5min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 27.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0764429931640624, 'learning_rate': 8.542274052478134e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0620723876953124, 'learning_rate': 7.08454810495627e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.062134765625, 'learning_rate': 5.626822157434403e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.042786376953125, 'learning_rate': 4.1690962099125366e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0551962890625, 'learning_rate': 2.7113702623906705e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04329345703125, 'learning_rate': 1.2536443148688048e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0989385986328124, 'learning_rate': 8.542274052478134e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0586090087890625, 'learning_rate': 7.08454810495627e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.057703369140625, 'learning_rate': 5.626822157434403e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.05137548828125, 'learning_rate': 4.1690962099125366e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0471904296875, 'learning_rate': 2.7113702623906705e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04178515625, 'learning_rate': 1.2536443148688048e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.153, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 11.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.10276025390625, 'learning_rate': 8.542274052478134e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.090162841796875, 'learning_rate': 7.08454810495627e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.079825927734375, 'learning_rate': 5.626822157434403e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.06939990234375, 'learning_rate': 4.1690962099125366e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.05262744140625, 'learning_rate': 2.7113702623906705e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.0617705078125, 'learning_rate': 1.2536443148688048e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.153, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0974371337890625, 'learning_rate': 8.542274052478134e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0936380615234376, 'learning_rate': 7.08454810495627e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.07395263671875, 'learning_rate': 5.626822157434403e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.049650390625, 'learning_rate': 4.1690962099125366e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.05623876953125, 'learning_rate': 2.7113702623906705e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.03886572265625, 'learning_rate': 1.2536443148688048e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.242, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.073612548828125, 'learning_rate': 8.542274052478134e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0978017578125, 'learning_rate': 7.08454810495627e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.074471435546875, 'learning_rate': 5.626822157434403e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.07447314453125, 'learning_rate': 4.1690962099125366e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0431787109375, 'learning_rate': 2.7113702623906705e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.0462587890625, 'learning_rate': 1.2536443148688048e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.1531449101061927, learning_rate=0.0001, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.155, total= 5.6min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 28.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.5979151000976562, 'learning_rate': 2.980860917248832e-05, 'epoch': 5.813953488372093}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.545, total= 3.4min\n",
            "[CV] attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.43985409545898435, 'learning_rate': 2.980860917248832e-05, 'epoch': 5.813953488372093}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.740, total= 3.4min\n",
            "[CV] attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.7min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.3979146423339844, 'learning_rate': 2.980860917248832e-05, 'epoch': 5.813953488372093}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.622, total= 3.4min\n",
            "[CV] attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.5033441772460937, 'learning_rate': 2.980860917248832e-05, 'epoch': 5.813953488372093}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.641, total= 3.4min\n",
            "[CV] attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.48504736328125, 'learning_rate': 2.980860917248832e-05, 'epoch': 5.813953488372093}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5485102676881799, hidden_dropout_prob=0.0, learning_rate=7.120945524538877e-05, num_train_epochs=10, per_device_train_batch_size=16, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.678, total= 3.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 16.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.297, total= 3.2min\n",
            "[CV] attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.297, total= 3.1min\n",
            "[CV] attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.3min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.296, total= 3.2min\n",
            "[CV] attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.320, total= 3.2min\n",
            "[CV] attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5505670976126504, hidden_dropout_prob=0.05908245884480623, learning_rate=1e-06, num_train_epochs=10, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.293, total= 3.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.928730224609375, 'learning_rate': 3.809552018192099e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7583128662109375, 'learning_rate': 3.159457817135427e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4816934814453125, 'learning_rate': 2.5093636160787546e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.308885986328125, 'learning_rate': 1.8592694150220825e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.142725830078125, 'learning_rate': 1.2091752139654102e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.075039794921875, 'learning_rate': 5.590810129087381e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.665, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.4min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8965134887695313, 'learning_rate': 3.809552018192099e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7119526977539062, 'learning_rate': 3.159457817135427e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5003778076171875, 'learning_rate': 2.5093636160787546e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.328534912109375, 'learning_rate': 1.8592694150220825e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.173434326171875, 'learning_rate': 1.2091752139654102e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.071444580078125, 'learning_rate': 5.590810129087381e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.716, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.8min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9468153076171875, 'learning_rate': 3.809552018192099e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7642442626953125, 'learning_rate': 3.159457817135427e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.465255126953125, 'learning_rate': 2.5093636160787546e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.27895361328125, 'learning_rate': 1.8592694150220825e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.17352099609375, 'learning_rate': 1.2091752139654102e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0810673828125, 'learning_rate': 5.590810129087381e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.643, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.87654052734375, 'learning_rate': 3.809552018192099e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.683949462890625, 'learning_rate': 3.159457817135427e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.3759061279296875, 'learning_rate': 2.5093636160787546e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.1771209716796875, 'learning_rate': 1.8592694150220825e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.096230712890625, 'learning_rate': 1.2091752139654102e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0425478515625, 'learning_rate': 5.590810129087381e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.631, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8985631103515626, 'learning_rate': 3.809552018192099e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7414349365234375, 'learning_rate': 3.159457817135427e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.458435791015625, 'learning_rate': 2.5093636160787546e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.258039306640625, 'learning_rate': 1.8592694150220825e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.134004638671875, 'learning_rate': 1.2091752139654102e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.053185791015625, 'learning_rate': 5.590810129087381e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5609426282257046, hidden_dropout_prob=0.0, learning_rate=4.459646219248771e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.707, total= 5.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 27.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.394, total=  36.0s\n",
            "[CV] attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.0s remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.408, total=  35.7s\n",
            "[CV] attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.419, total=  35.7s\n",
            "[CV] attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.420, total=  35.9s\n",
            "[CV] attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4306134471329855, hidden_dropout_prob=0.0, learning_rate=4.6566554126392125e-05, num_train_epochs=1, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.420, total= 1.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9385889282226563, 'learning_rate': 3.380321982825478e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7461533813476563, 'learning_rate': 2.8034752280771032e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.5861463623046875, 'learning_rate': 2.226628473328728e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.4597294921875, 'learning_rate': 1.649781718580353e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.28113818359375, 'learning_rate': 1.0729349638319777e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.142553466796875, 'learning_rate': 4.960882090836027e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.580, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8377970581054688, 'learning_rate': 3.380321982825478e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6105617065429687, 'learning_rate': 2.8034752280771032e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.315281982421875, 'learning_rate': 2.226628473328728e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.16090625, 'learning_rate': 1.649781718580353e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.0462977294921875, 'learning_rate': 1.0729349638319777e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0110638427734375, 'learning_rate': 4.960882090836027e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.717, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.9min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8787210083007813, 'learning_rate': 3.380321982825478e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6044907836914063, 'learning_rate': 2.8034752280771032e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.291861572265625, 'learning_rate': 2.226628473328728e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.1618255615234375, 'learning_rate': 1.649781718580353e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.063600830078125, 'learning_rate': 1.0729349638319777e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0043477783203125, 'learning_rate': 4.960882090836027e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.649, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8787764892578125, 'learning_rate': 3.380321982825478e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6248033447265625, 'learning_rate': 2.8034752280771032e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.353352783203125, 'learning_rate': 2.226628473328728e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.163471923828125, 'learning_rate': 1.649781718580353e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.071937255859375, 'learning_rate': 1.0729349638319777e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.030339111328125, 'learning_rate': 4.960882090836027e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.649, total= 5.4min\n",
            "[CV] attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8611641845703125, 'learning_rate': 3.380321982825478e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7359736328125, 'learning_rate': 2.8034752280771032e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.3899293212890625, 'learning_rate': 2.226628473328728e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.154192138671875, 'learning_rate': 1.649781718580353e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.08099658203125, 'learning_rate': 1.0729349638319777e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.050704345703125, 'learning_rate': 4.960882090836027e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.4946962679820598, hidden_dropout_prob=0.0, learning_rate=3.9571687375738534e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.667, total= 5.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 27.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.07624609375, 'learning_rate': 6.902862518739578e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.05771142578125, 'learning_rate': 5.7248996315826544e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.05904248046875, 'learning_rate': 4.5469367444257294e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.05082373046875, 'learning_rate': 3.368973857268805e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.05059033203125, 'learning_rate': 2.1910109701118796e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04964208984375, 'learning_rate': 1.0130480829549553e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.152, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0740211181640624, 'learning_rate': 6.902862518739578e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0605758056640624, 'learning_rate': 5.7248996315826544e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.060532470703125, 'learning_rate': 4.5469367444257294e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.0503720703125, 'learning_rate': 3.368973857268805e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.050845703125, 'learning_rate': 2.1910109701118796e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04433447265625, 'learning_rate': 1.0130480829549553e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 11.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.09622998046875, 'learning_rate': 6.902862518739578e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.065376708984375, 'learning_rate': 5.7248996315826544e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.050794921875, 'learning_rate': 4.5469367444257294e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.060814208984375, 'learning_rate': 3.368973857268805e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.04836083984375, 'learning_rate': 2.1910109701118796e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.0480185546875, 'learning_rate': 1.0130480829549553e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.317, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0855374755859375, 'learning_rate': 6.902862518739578e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0691180419921875, 'learning_rate': 5.7248996315826544e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.0617890625, 'learning_rate': 4.5469367444257294e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.050035400390625, 'learning_rate': 3.368973857268805e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.054794921875, 'learning_rate': 2.1910109701118796e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.03597509765625, 'learning_rate': 1.0130480829549553e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.155, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.071425537109375, 'learning_rate': 6.902862518739578e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.072355712890625, 'learning_rate': 5.7248996315826544e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.058560302734375, 'learning_rate': 4.5469367444257294e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.055194091796875, 'learning_rate': 3.368973857268805e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0455595703125, 'learning_rate': 2.1910109701118796e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04616748046875, 'learning_rate': 1.0130480829549553e-05, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5289975614009103, hidden_dropout_prob=0.10086717174739536, learning_rate=8.080825405896503e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.153, total= 5.6min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 28.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.539, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.654, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.553, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.604, total= 2.5min\n",
            "[CV] attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 8.0}\n",
            "[CV]  attention_probs_dropout_prob=0.00425392459679252, hidden_dropout_prob=0.017208961715406236, learning_rate=3.7408016305073955e-05, num_train_epochs=8, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.637, total= 2.5min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.066190185546875, 'learning_rate': 3.54351482643156e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.047540283203125, 'learning_rate': 2.9388194635592803e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.042002197265625, 'learning_rate': 2.3341241006870005e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.037540771484375, 'learning_rate': 1.7294287378147206e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0388828125, 'learning_rate': 1.1247333749424406e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.03704541015625, 'learning_rate': 5.200380120701608e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.6min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0666702880859376, 'learning_rate': 3.54351482643156e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0526925048828124, 'learning_rate': 2.9388194635592803e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.056194580078125, 'learning_rate': 2.3341241006870005e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.055684814453125, 'learning_rate': 1.7294287378147206e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.0557587890625, 'learning_rate': 1.1247333749424406e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.033478515625, 'learning_rate': 5.200380120701608e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.156, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 11.2min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0788060302734375, 'learning_rate': 3.54351482643156e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0502347412109374, 'learning_rate': 2.9388194635592803e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.047150634765625, 'learning_rate': 2.3341241006870005e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.05912646484375, 'learning_rate': 1.7294287378147206e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.04002294921875, 'learning_rate': 1.1247333749424406e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.04771484375, 'learning_rate': 5.200380120701608e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.167, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0653226318359375, 'learning_rate': 3.54351482643156e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0649564208984375, 'learning_rate': 2.9388194635592803e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.044835205078125, 'learning_rate': 2.3341241006870005e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.0515322265625, 'learning_rate': 1.7294287378147206e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.04205908203125, 'learning_rate': 1.1247333749424406e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.02607421875, 'learning_rate': 5.200380120701608e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.162, total= 5.6min\n",
            "[CV] attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 1.0626864013671875, 'learning_rate': 3.54351482643156e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 1.0648590087890626, 'learning_rate': 2.9388194635592803e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 1.050308349609375, 'learning_rate': 2.3341241006870005e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 1.046664794921875, 'learning_rate': 1.7294287378147206e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 1.03724658203125, 'learning_rate': 1.1247333749424406e-05, 'epoch': 7.288629737609329}\n",
            "{'loss': 1.035388671875, 'learning_rate': 5.200380120701608e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.9, hidden_dropout_prob=0.007168347355307692, learning_rate=4.14821018930384e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.155, total= 5.6min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 28.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.891261474609375, 'learning_rate': 2.7695305548848594e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.711755126953125, 'learning_rate': 2.2969144192389792e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4674609375, 'learning_rate': 1.8242982835930987e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.239456787109375, 'learning_rate': 1.3516821479472181e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.142550537109375, 'learning_rate': 8.790660123013376e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.07103076171875, 'learning_rate': 4.0644987665545726e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.649, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8847770385742187, 'learning_rate': 2.7695305548848594e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.7331024780273437, 'learning_rate': 2.2969144192389792e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4540281982421875, 'learning_rate': 1.8242982835930987e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.25620947265625, 'learning_rate': 1.3516821479472181e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.13089013671875, 'learning_rate': 8.790660123013376e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.074353515625, 'learning_rate': 4.0644987665545726e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.691, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.9min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8982708129882813, 'learning_rate': 2.7695305548848594e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6198584594726563, 'learning_rate': 2.2969144192389792e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.33041015625, 'learning_rate': 1.8242982835930987e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.2063953857421875, 'learning_rate': 1.3516821479472181e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.094479248046875, 'learning_rate': 8.790660123013376e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0477451171875, 'learning_rate': 4.0644987665545726e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.698, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.9085072631835938, 'learning_rate': 2.7695305548848594e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6814048461914063, 'learning_rate': 2.2969144192389792e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.4446671142578125, 'learning_rate': 1.8242982835930987e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.2257677001953125, 'learning_rate': 1.3516821479472181e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.127534423828125, 'learning_rate': 8.790660123013376e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.074672119140625, 'learning_rate': 4.0644987665545726e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.549, total= 5.5min\n",
            "[CV] attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.8305989379882812, 'learning_rate': 2.7695305548848594e-05, 'epoch': 1.4577259475218658}\n",
            "{'loss': 0.6307247924804688, 'learning_rate': 2.2969144192389792e-05, 'epoch': 2.9154518950437316}\n",
            "{'loss': 0.3372464599609375, 'learning_rate': 1.8242982835930987e-05, 'epoch': 4.373177842565598}\n",
            "{'loss': 0.1381864013671875, 'learning_rate': 1.3516821479472181e-05, 'epoch': 5.830903790087463}\n",
            "{'loss': 0.0722908935546875, 'learning_rate': 8.790660123013376e-06, 'epoch': 7.288629737609329}\n",
            "{'loss': 0.0218458251953125, 'learning_rate': 4.0644987665545726e-06, 'epoch': 8.746355685131196}\n",
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.5582518438325175, hidden_dropout_prob=0.01113854293589334, learning_rate=3.24214669053074e-05, num_train_epochs=10, per_device_train_batch_size=4, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.720, total= 5.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 27.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.494, total= 1.9min\n",
            "[CV] attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.410, total= 1.9min\n",
            "[CV] attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.8min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.475, total= 1.9min\n",
            "[CV] attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.447, total= 1.9min\n",
            "[CV] attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 6.0}\n",
            "[CV]  attention_probs_dropout_prob=0.594666189808815, hidden_dropout_prob=0.0036390475755954596, learning_rate=3.5833975465238777e-05, num_train_epochs=6, per_device_train_batch_size=32, pretrained_model=dccuchile/bert-base-spanish-wwm-uncased, score=0.414, total= 1.9min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.7121862182617188, 'learning_rate': 6.132838263254131e-05, 'epoch': 2.9069767441860463}\n",
            "{'loss': 0.33038348388671873, 'learning_rate': 3.206865809411511e-05, 'epoch': 5.813953488372093}\n",
            "{'loss': 0.08090625, 'learning_rate': 2.808933555688915e-06, 'epoch': 8.720930232558139}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.624, total= 3.5min\n",
            "[CV] attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.7615147094726562, 'learning_rate': 6.132838263254131e-05, 'epoch': 2.9069767441860463}\n",
            "{'loss': 0.4382083129882812, 'learning_rate': 3.206865809411511e-05, 'epoch': 5.813953488372093}\n",
            "{'loss': 0.24873095703125, 'learning_rate': 2.808933555688915e-06, 'epoch': 8.720930232558139}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.496, total= 3.5min\n",
            "[CV] attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.1min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.6813892211914062, 'learning_rate': 6.132838263254131e-05, 'epoch': 2.9069767441860463}\n",
            "{'loss': 0.20207513427734375, 'learning_rate': 3.206865809411511e-05, 'epoch': 5.813953488372093}\n",
            "{'loss': 0.03752581787109375, 'learning_rate': 2.808933555688915e-06, 'epoch': 8.720930232558139}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.620, total= 3.5min\n",
            "[CV] attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.7203907470703125, 'learning_rate': 6.132838263254131e-05, 'epoch': 2.9069767441860463}\n",
            "{'loss': 0.2709984130859375, 'learning_rate': 3.206865809411511e-05, 'epoch': 5.813953488372093}\n",
            "{'loss': 0.04492236328125, 'learning_rate': 2.808933555688915e-06, 'epoch': 8.720930232558139}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.524, total= 3.5min\n",
            "[CV] attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.6601710205078125, 'learning_rate': 6.132838263254131e-05, 'epoch': 2.9069767441860463}\n",
            "{'loss': 0.17030615234375, 'learning_rate': 3.206865809411511e-05, 'epoch': 5.813953488372093}\n",
            "{'loss': 0.0173367919921875, 'learning_rate': 2.808933555688915e-06, 'epoch': 8.720930232558139}\n",
            "{'epoch': 9.0}\n",
            "[CV]  attention_probs_dropout_prob=0.11877989840278479, hidden_dropout_prob=0.0, learning_rate=9.05881071709675e-05, num_train_epochs=9, per_device_train_batch_size=8, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.617, total= 3.5min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.401, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.444, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.0min remaining:    0.0s\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.443, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.409, total= 3.0min\n",
            "[CV] attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 10.0}\n",
            "[CV]  attention_probs_dropout_prob=0.6093773596951091, hidden_dropout_prob=0.027175797976312522, learning_rate=2.007004297760599e-05, num_train_epochs=10, per_device_train_batch_size=64, pretrained_model=dccuchile/bert-base-spanish-wwm-cased, score=0.410, total= 3.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.1min finished\n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.874583251953125, 'learning_rate': 3.9386595113926067e-05, 'epoch': 1.1682242990654206}\n",
            "{'loss': 0.6801448974609375, 'learning_rate': 3.4176728035364416e-05, 'epoch': 2.336448598130841}\n",
            "{'loss': 0.5423587646484375, 'learning_rate': 2.8966860956802765e-05, 'epoch': 3.5046728971962615}\n",
            "{'loss': 0.387064208984375, 'learning_rate': 2.3756993878241114e-05, 'epoch': 4.672897196261682}\n",
            "{'loss': 0.230538818359375, 'learning_rate': 1.8547126799679467e-05, 'epoch': 5.841121495327103}\n",
            "{'loss': 0.13231640625, 'learning_rate': 1.3337259721117818e-05, 'epoch': 7.009345794392523}\n",
            "{'loss': 0.064142333984375, 'learning_rate': 8.12739264255617e-06, 'epoch': 8.177570093457945}\n",
            "{'loss': 0.05944580078125, 'learning_rate': 2.9175255639945227e-06, 'epoch': 9.345794392523365}\n",
            "{'epoch': 10.0}\n",
            "CPU times: user 5h 35min 27s, sys: 1h 38min 25s, total: 7h 13min 53s\n",
            "Wall time: 7h 24min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFhtRn4DqxlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176092e8-c419-4f1c-e67f-01e0b1447095"
      },
      "source": [
        "metamodel.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('attention_probs_dropout_prob', 0.5609426282257046),\n",
              "             ('hidden_dropout_prob', 0.0),\n",
              "             ('learning_rate', 4.459646219248771e-05),\n",
              "             ('num_train_epochs', 10),\n",
              "             ('per_device_train_batch_size', 4),\n",
              "             ('pretrained_model', 'dccuchile/bert-base-spanish-wwm-cased')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbj-1s_Cq1nP"
      },
      "source": [
        "Optimized model metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzmECC9Qq2Mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69aee09-d635-467d-9fa4-33d4d860ddd0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = metamodel.predict(X_test)\n",
        "print(classification_report(Y_test, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       173\n",
            "           1       0.75      0.58      0.65        31\n",
            "           2       0.29      0.29      0.29        14\n",
            "           3       0.90      0.87      0.89       210\n",
            "\n",
            "    accuracy                           0.82       428\n",
            "   macro avg       0.68      0.65      0.66       428\n",
            "weighted avg       0.82      0.82      0.82       428\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS2BZSb_q3XE"
      },
      "source": [
        "Best optimized result\n",
        "\n",
        "    OrderedDict([('attention_probs_dropout_prob', 0.371179371670504),\n",
        "             ('hidden_dropout_prob', 0.0),\n",
        "             ('learning_rate', 8.835158172392285e-05),\n",
        "             ('num_train_epochs', 6),\n",
        "             ('per_device_train_batch_size', 32),\n",
        "             ('pretrained_model', 'dccuchile/bert-base-spanish-wwm-cased')])\n",
        "\n",
        "                precision    recall  f1-score   support\n",
        "\n",
        "              0       0.75      0.89      0.82       173\n",
        "              1       0.71      0.65      0.68        31\n",
        "              2       0.50      0.36      0.42        14\n",
        "              3       0.95      0.84      0.89       210\n",
        "\n",
        "        accuracy                           0.83       428\n",
        "      macro avg       0.73      0.68      0.70       428\n",
        "    weighted avg       0.84      0.83      0.83       428\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSwBZLrkq5Zu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNBrqJVHDoWG"
      },
      "source": [
        "### 2.6. Basic embeddings model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0FX_jzuDuEL"
      },
      "source": [
        "For reference we also provide experimental results with a simple model making use of pre-trained word embeddings. This model represents a text as the average of the word embeddings of every word in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW2OP8lIDoWL"
      },
      "source": [
        "Install spacy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJL3I593Essq",
        "outputId": "003c84b8-0cc0-4ae6-80b3-3cd5a8d7f744"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-07 10:20:21.230153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Collecting en-core-web-lg==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778.8MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-lg==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (53.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.14)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.0.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytYSnV28I77f"
      },
      "source": [
        "Transform texts to embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9-4vNsBHETk"
      },
      "source": [
        "import numpy as np\r\n",
        "import spacy\r\n",
        "\r\n",
        "def vectorize_texts(texts, spacy_model='en_core_web_lg'):\r\n",
        "    nlp = spacy.load(spacy_model)\r\n",
        "    return np.array([vectorize_text(text, nlp) for text in texts])\r\n",
        "\r\n",
        "def vectorize_text(text, nlp):\r\n",
        "    tokens = nlp(text)\r\n",
        "    vector = np.zeros(tokens[0].vector.shape)\r\n",
        "    n_vectors = 0\r\n",
        "    for token in tokens:\r\n",
        "        if token.has_vector:\r\n",
        "            vector += token.vector\r\n",
        "            n_vectors +=1\r\n",
        "    return vector / n_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OfLpeZwIMAD"
      },
      "source": [
        "E_train = vectorize_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1YZUoK7Iu24"
      },
      "source": [
        "E_train_oversampling = vectorize_texts(X_train_oversampling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxtlb7ntIz7-"
      },
      "source": [
        "E_test = vectorize_texts(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lglBDOgNKFf9"
      },
      "source": [
        "#### 2.6.1. SpacyEmbeddings + RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r9o9YBqJGmg",
        "outputId": "b92298f8-7a55-494c-92cf-7db36d4d962a"
      },
      "source": [
        "spacy_rf_model = RandomForestClassifier(random_state=42, n_jobs = -1)\r\n",
        "\r\n",
        "params = {\r\n",
        "    'n_estimators' : RF_ESTIMATORS,\r\n",
        "}\r\n",
        "\r\n",
        "spacy_rf_gs_model = RandomizedSearchCV(spacy_rf_model, params, n_jobs = -1, cv=StratifiedKFold(), n_iter=TUNING_ITERATIONS, random_state=12345, verbose=2)\r\n",
        "\r\n",
        "spacy_rf_gs_model.fit(E_train, Y_train)\r\n",
        "\r\n",
        "print(spacy_rf_gs_model.best_params_)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 3 is smaller than n_iter=30. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5rubycSKbtO",
        "outputId": "431f674d-bfd0-4e84-c55d-d1a0873e168a"
      },
      "source": [
        "spacy_rf_gs_preds = spacy_rf_gs_model.predict(E_test)\r\n",
        "np.save('preds/spacy_rf_gs_preds', spacy_rf_gs_preds)\r\n",
        "print(classification_report(Y_test, spacy_rf_gs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.59      0.55       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.59      0.64      0.62       210\n",
            "\n",
            "    accuracy                           0.55       428\n",
            "   macro avg       0.28      0.31      0.29       428\n",
            "weighted avg       0.50      0.55      0.52       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLVD0_I2Neyu"
      },
      "source": [
        "Very poor results for this modelling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnkNzCnMPT6B"
      },
      "source": [
        "### 2.7. Word embeddings + recurrent mixing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fwGX14LPT6E"
      },
      "source": [
        "We further explore the previous results by devoloping a more advanced embeddings-based model: mixing word embeddings through a Gated Recurrent Unit layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOuX4Vy6e4ki"
      },
      "source": [
        "First, load fasttext embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8i8DsibSyC1",
        "outputId": "86ba05ef-7c80-4bf5-d12a-caa8bd2c3b35"
      },
      "source": [
        "import fasttext.util\r\n",
        "\r\n",
        "fasttext.util.download_model('es', if_exists='ignore')\r\n",
        "ft = fasttext.load_model('cc.es.300.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxJOdK1he8Lm"
      },
      "source": [
        "The following code defines a model with fasttext embeddings followed by a GRU mixing layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Y7j_nCW6Qz"
      },
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.layers import Bidirectional, Dense, GlobalAveragePooling1D, GRU, Input, SpatialDropout1D, Dropout\r\n",
        "from keras.preprocessing.sequence import pad_sequences \r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "class FastTextGRUClassifier(BaseEstimator, ClassifierMixin):\r\n",
        "    \"\"\"Model implementing FastText embeddings for words followed by a GRU recurrent deep network\"\"\"\r\n",
        "    def __init__(self, spatial_dropout=0.2, gru_layers=1, gru_units=80, gru_dropout=0.0, dense_layers=1, dense_units=16, dense_dropout=0.0,\r\n",
        "                 max_tokenizer_words=15000, max_sequence=45, epochs=1, batch_size=64):\r\n",
        "        self.spatial_dropout = spatial_dropout\r\n",
        "        self.gru_layers = gru_layers\r\n",
        "        self.gru_units = gru_units\r\n",
        "        self.gru_dropout = gru_dropout\r\n",
        "        self.dense_layers = dense_layers\r\n",
        "        self.dense_units = dense_units\r\n",
        "        self.dense_dropout = dense_dropout\r\n",
        "        self.max_tokenizer_words = max_tokenizer_words\r\n",
        "        self.max_sequence = max_sequence\r\n",
        "        self.epochs = epochs\r\n",
        "        self.batch_size = batch_size\r\n",
        "\r\n",
        "    def _create_embedding_matrix(self, fasttext_model):\r\n",
        "        \"\"\"Creates a weight matrix for an Embedding layer using a fasttext_model and a Tokenizer\"\"\"\r\n",
        "        \r\n",
        "        # Compute mean and standard deviation for words embeddings\r\n",
        "        all_embs = np.stack([\r\n",
        "            fasttext_model.get_word_vector(word) for word in self._tokenizer.word_index.keys()\r\n",
        "        ])\r\n",
        "\r\n",
        "        emb_mean, emb_std = all_embs.mean(), all_embs.std()\r\n",
        "        \r\n",
        "        embedding_size = fasttext_model.get_dimension()\r\n",
        "        \r\n",
        "        embedding_matrix = np.random.normal(emb_mean, emb_std, (self._tokenizer.num_words, embedding_size))\r\n",
        "        for word, i in self._tokenizer.word_index.items():\r\n",
        "            if i >= self._tokenizer.num_words: break\r\n",
        "            embedding_vector = fasttext_model.get_word_vector(word)\r\n",
        "            if embedding_vector is not None: \r\n",
        "                embedding_matrix[i] = embedding_vector\r\n",
        "                \r\n",
        "        return embedding_matrix\r\n",
        "\r\n",
        "    def fit(self, X, y, fasttext_model):\r\n",
        "        # Prepare tokenizer\r\n",
        "        self._tokenizer = Tokenizer(num_words=self.max_tokenizer_words)\r\n",
        "        self._tokenizer.fit_on_texts(X)\r\n",
        "\r\n",
        "        # Transform texts to sequences\r\n",
        "        tokenized = self._tokenizer.texts_to_sequences(X)\r\n",
        "        sequences = pad_sequences(tokenized, maxlen=self.max_sequence)\r\n",
        "\r\n",
        "        # One-hot encode output labels\r\n",
        "        y_hot = to_categorical(y)\r\n",
        "\r\n",
        "        # Build embeddings matrix\r\n",
        "        embedding_matrix = self._create_embedding_matrix(fasttext_model)\r\n",
        "\r\n",
        "        # Build neural network\r\n",
        "        inp = Input(shape=(self.max_sequence, ))\r\n",
        "        x = Embedding(self.max_tokenizer_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inp)\r\n",
        "        x = SpatialDropout1D(self.spatial_dropout)(x)\r\n",
        "        x = Bidirectional(GRU(self.gru_units, dropout=self.gru_dropout, return_sequences=True))(x)\r\n",
        "        for _ in range(1, self.gru_layers):\r\n",
        "            x = GRU(self.gru_units, dropout=self.gru_dropout, return_sequences=True)(x)\r\n",
        "        x = GlobalAveragePooling1D()(x)\r\n",
        "        for _ in range(self.dense_layers):\r\n",
        "            x = Dense(self.dense_units, activation=\"relu\")(x)\r\n",
        "            x = Dropout(self.dense_dropout)(x)\r\n",
        "        x = Dense(len(set(y)), activation=\"softmax\")(x)\r\n",
        "        model = Model(inputs=inp, outputs=x)\r\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "        model.fit(sequences, y_hot, batch_size=self.batch_size, epochs=self.epochs, verbose=0)\r\n",
        "        self._model = model\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        # Transform texts to sequences\r\n",
        "        tokenized = self._tokenizer.texts_to_sequences(X)\r\n",
        "        sequences = pad_sequences(tokenized, maxlen=self.max_sequence)\r\n",
        "\r\n",
        "        # Predict with model\r\n",
        "        preds = self._model.predict(sequences)\r\n",
        "        return np.argmax(preds, axis=1)\r\n",
        "\r\n",
        "    def score(self, X, y):\r\n",
        "        preds = self.predict(X)\r\n",
        "        return f1_score(y, preds, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmi_GBhpCfZX"
      },
      "source": [
        "Basic GRU (default parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-nRkfSaK9_"
      },
      "source": [
        "gru_model = FastTextGRUClassifier(epochs=20)\r\n",
        "gru_model.fit(X_train, Y_train, ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Lduy7QRh0e",
        "outputId": "d39ad4be-2b00-437d-cc99-9bfefb6fb01d"
      },
      "source": [
        "gru_preds = gru_model.predict(X_test)\r\n",
        "np.save('preds/gru_preds', gru_preds)\r\n",
        "print(classification_report(Y_test, gru_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.68      0.63       173\n",
            "           1       0.00      0.00      0.00        31\n",
            "           2       0.00      0.00      0.00        14\n",
            "           3       0.70      0.76      0.73       210\n",
            "\n",
            "    accuracy                           0.65       428\n",
            "   macro avg       0.32      0.36      0.34       428\n",
            "weighted avg       0.58      0.65      0.61       428\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K16mb2LkgB_e"
      },
      "source": [
        "Hyperoptimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1SAl5rVJXP",
        "outputId": "c0904ef0-35e5-4a51-9f7e-1fcd6c9e0efc"
      },
      "source": [
        "%%time\r\n",
        "from skopt import BayesSearchCV\r\n",
        "from skopt.space.space import Integer, Real\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "\r\n",
        "param_grid = {\r\n",
        "    \"spatial_dropout\": Real(0, 0.9, \"uniform\"),\r\n",
        "    \"gru_layers\": [1, 2, 3],\r\n",
        "    \"gru_units\": [16, 32, 64, 128, 256, 512, 1024],\r\n",
        "    \"gru_dropout\": Real(0, 0.9, \"uniform\"),\r\n",
        "    \"dense_layers\": [1, 2, 3],\r\n",
        "    \"dense_units\" : [16, 32, 64, 128, 256, 512, 1024],\r\n",
        "    \"dense_dropout\": Real(0, 0.9, \"uniform\"),\r\n",
        "    \"epochs\": Integer(50, 200, \"uniform\"),\r\n",
        "}\r\n",
        "\r\n",
        "cv_method = StratifiedKFold()\r\n",
        "\r\n",
        "gru_bs_model = BayesSearchCV(FastTextGRUClassifier(), param_grid, n_iter=30, verbose=3, cv=cv_method, random_state=12345, error_score=0.0, fit_params={\"fasttext_model\": ft})\r\n",
        "gru_bs_model.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796, score=0.286, total=  37.2s\n",
            "[CV] dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796, score=0.316, total=  37.3s\n",
            "[CV] dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796, score=0.317, total=  36.9s\n",
            "[CV] dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796 \n",
            "[CV]  dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796, score=0.326, total=  38.3s\n",
            "[CV] dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796 \n",
            "[CV]  dense_dropout=0.36712704740889385, dense_layers=3, dense_units=256, epochs=109, gru_dropout=0.7118832782946124, gru_layers=3, gru_units=64, spatial_dropout=0.5377325391546796, score=0.328, total=  38.1s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391, score=0.307, total=  16.7s\n",
            "[CV] dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391, score=0.318, total=  16.8s\n",
            "[CV] dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   33.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391, score=0.315, total=  17.1s\n",
            "[CV] dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391 \n",
            "[CV]  dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391, score=0.330, total=  16.2s\n",
            "[CV] dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391 \n",
            "[CV]  dense_dropout=0.4626460576204746, dense_layers=2, dense_units=64, epochs=70, gru_dropout=0.13937691274336583, gru_layers=1, gru_units=32, spatial_dropout=0.3450716700768391, score=0.332, total=  17.1s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.4min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135, score=0.323, total=  34.9s\n",
            "[CV] dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   34.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135, score=0.327, total=  34.8s\n",
            "[CV] dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135, score=0.332, total=  34.9s\n",
            "[CV] dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135 \n",
            "[CV]  dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135, score=0.319, total=  34.4s\n",
            "[CV] dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135 \n",
            "[CV]  dense_dropout=0.041508544006946244, dense_layers=2, dense_units=1024, epochs=130, gru_dropout=0.37781661913068065, gru_layers=2, gru_units=32, spatial_dropout=0.697509239293135, score=0.314, total=  34.9s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.9min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894, score=0.312, total=  55.4s\n",
            "[CV] dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   55.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894, score=0.336, total=  55.2s\n",
            "[CV] dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894, score=0.323, total=  55.1s\n",
            "[CV] dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894 \n",
            "[CV]  dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894, score=0.341, total=  55.9s\n",
            "[CV] dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894 \n",
            "[CV]  dense_dropout=0.22314680177365268, dense_layers=3, dense_units=256, epochs=176, gru_dropout=0.44157822614380576, gru_layers=3, gru_units=32, spatial_dropout=0.0033130250892628894, score=0.298, total=  55.4s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066, score=0.156, total=  35.9s\n",
            "[CV] dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066, score=0.156, total=  37.1s\n",
            "[CV] dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066, score=0.156, total=  36.6s\n",
            "[CV] dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066 \n",
            "[CV]  dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066, score=0.155, total=  36.7s\n",
            "[CV] dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066 \n",
            "[CV]  dense_dropout=0.8685989173436767, dense_layers=2, dense_units=16, epochs=107, gru_dropout=0.5328882997083877, gru_layers=3, gru_units=32, spatial_dropout=0.8449996785268066, score=0.155, total=  36.3s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.0min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556, score=0.294, total=  36.0s\n",
            "[CV] dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556, score=0.300, total=  36.4s\n",
            "[CV] dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556, score=0.310, total=  36.5s\n",
            "[CV] dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556 \n",
            "[CV]  dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556, score=0.300, total=  35.8s\n",
            "[CV] dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556 \n",
            "[CV]  dense_dropout=0.3691514693845362, dense_layers=3, dense_units=16, epochs=179, gru_dropout=0.6137651209585483, gru_layers=1, gru_units=32, spatial_dropout=0.7269519404781556, score=0.313, total=  36.2s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.0min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123, score=0.298, total=  31.2s\n",
            "[CV] dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123, score=0.307, total=  31.1s\n",
            "[CV] dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123, score=0.303, total=  31.4s\n",
            "[CV] dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123 \n",
            "[CV]  dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123, score=0.354, total=  30.7s\n",
            "[CV] dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123 \n",
            "[CV]  dense_dropout=0.03317383878440064, dense_layers=2, dense_units=512, epochs=86, gru_dropout=0.7415784521982055, gru_layers=3, gru_units=64, spatial_dropout=0.17930928606619123, score=0.313, total=  30.7s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371, score=0.242, total=  19.7s\n",
            "[CV] dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371, score=0.272, total=  19.1s\n",
            "[CV] dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   38.8s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371, score=0.289, total=  19.4s\n",
            "[CV] dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371 \n",
            "[CV]  dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371, score=0.265, total=  19.3s\n",
            "[CV] dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371 \n",
            "[CV]  dense_dropout=0.8431661252980369, dense_layers=2, dense_units=256, epochs=60, gru_dropout=0.5349015985256025, gru_layers=2, gru_units=16, spatial_dropout=0.8088179031533371, score=0.285, total=  19.2s\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123, score=0.319, total=  29.6s\n",
            "[CV] dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123, score=0.331, total=  29.6s\n",
            "[CV] dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   59.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123, score=0.316, total=  29.6s\n",
            "[CV] dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123 \n",
            "[CV]  dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123, score=0.319, total=  29.3s\n",
            "[CV] dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123 \n",
            "[CV]  dense_dropout=0.21035384276262642, dense_layers=2, dense_units=128, epochs=106, gru_dropout=0.5565299474116303, gru_layers=2, gru_units=32, spatial_dropout=0.43430233522902123, score=0.300, total=  29.6s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314, score=0.292, total= 1.1min\n",
            "[CV] dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314, score=0.309, total= 1.1min\n",
            "[CV] dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314, score=0.318, total= 1.1min\n",
            "[CV] dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314 \n",
            "[CV]  dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314, score=0.323, total= 1.1min\n",
            "[CV] dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314 \n",
            "[CV]  dense_dropout=0.6629296110884636, dense_layers=2, dense_units=32, epochs=166, gru_dropout=0.6431953593736593, gru_layers=2, gru_units=256, spatial_dropout=0.42356806707829314, score=0.330, total= 1.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717, score=0.279, total=  39.4s\n",
            "[CV] dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717, score=0.359, total=  39.9s\n",
            "[CV] dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717, score=0.316, total=  39.4s\n",
            "[CV] dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717 \n",
            "[CV]  dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717, score=0.369, total=  39.9s\n",
            "[CV] dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717 \n",
            "[CV]  dense_dropout=0.6162658467335622, dense_layers=2, dense_units=256, epochs=144, gru_dropout=0.5062365776729225, gru_layers=1, gru_units=256, spatial_dropout=0.2366403623296717, score=0.338, total=  39.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873, score=0.274, total=  59.8s\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   59.8s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873, score=0.320, total=  59.4s\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873, score=0.283, total= 1.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873, score=0.351, total=  59.6s\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=512, epochs=112, gru_dropout=0.0, gru_layers=1, gru_units=512, spatial_dropout=0.856983285449873, score=0.287, total=  59.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.156, total= 1.0min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.273, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.222, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.155, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=16, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.229, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0, score=0.371, total= 8.4min\n",
            "[CV] dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0, score=0.379, total= 8.3min\n",
            "[CV] dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 16.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0, score=0.338, total= 8.3min\n",
            "[CV] dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0, score=0.371, total= 8.4min\n",
            "[CV] dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.8390663304905417, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=2, gru_units=1024, spatial_dropout=0.0, score=0.375, total= 8.3min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 41.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475, score=0.299, total= 1.2min\n",
            "[CV] dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475, score=0.316, total= 1.3min\n",
            "[CV] dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475, score=0.316, total= 1.2min\n",
            "[CV] dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475 \n",
            "[CV]  dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475, score=0.323, total= 1.3min\n",
            "[CV] dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475 \n",
            "[CV]  dense_dropout=0.7402548530860971, dense_layers=2, dense_units=1024, epochs=57, gru_dropout=0.4202306826684827, gru_layers=1, gru_units=1024, spatial_dropout=0.7253027831577475, score=0.317, total= 1.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  6.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.307, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.291, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.308, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.292, total= 1.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=50, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.334, total= 1.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0, score=0.323, total= 3.7min\n",
            "[CV] dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0, score=0.414, total= 3.7min\n",
            "[CV] dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0, score=0.350, total= 3.7min\n",
            "[CV] dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0, score=0.385, total= 3.7min\n",
            "[CV] dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.7038706820776923, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=512, spatial_dropout=0.0, score=0.351, total= 3.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0, score=0.325, total=  50.3s\n",
            "[CV] dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   50.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0, score=0.312, total=  50.2s\n",
            "[CV] dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0, score=0.349, total=  50.0s\n",
            "[CV] dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0, score=0.325, total=  50.8s\n",
            "[CV] dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.7681309803599284, dense_layers=2, dense_units=512, epochs=200, gru_dropout=0.6190918946917064, gru_layers=2, gru_units=16, spatial_dropout=0.0, score=0.306, total=  50.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0, score=0.322, total=11.2min\n",
            "[CV] dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 11.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0, score=0.335, total=11.1min\n",
            "[CV] dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 22.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0, score=0.284, total=11.1min\n",
            "[CV] dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0, score=0.393, total=11.1min\n",
            "[CV] dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.4449384648426246, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.0, score=0.321, total=11.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 55.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566, score=0.310, total=  38.1s\n",
            "[CV] dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566, score=0.327, total=  38.5s\n",
            "[CV] dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566, score=0.332, total=  38.6s\n",
            "[CV] dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566 \n",
            "[CV]  dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566, score=0.349, total=  38.8s\n",
            "[CV] dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566 \n",
            "[CV]  dense_dropout=0.0, dense_layers=2, dense_units=16, epochs=200, gru_dropout=0.7411463210929242, gru_layers=1, gru_units=16, spatial_dropout=0.27794733329881566, score=0.307, total=  38.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.300, total= 4.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.296, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.320, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.324, total= 4.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=64, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.310, total= 4.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 20.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754, score=0.348, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754, score=0.351, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754, score=0.327, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754, score=0.369, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.12258485064042754, score=0.330, total= 4.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 20.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505, score=0.308, total= 4.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505, score=0.338, total= 4.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505, score=0.360, total= 4.0min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505, score=0.417, total= 4.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=128, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.40775029884205505, score=0.355, total= 4.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 20.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502, score=0.318, total=11.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 11.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502, score=0.308, total=11.1min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 22.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502, score=0.334, total=11.2min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502, score=0.374, total=11.2min\n",
            "[CV] dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502 \n",
            "[CV]  dense_dropout=0.0, dense_layers=3, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=3, gru_units=1024, spatial_dropout=0.2649965115093502, score=0.315, total=11.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 55.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163, score=0.304, total= 4.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163, score=0.330, total= 4.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163, score=0.336, total= 4.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163, score=0.341, total= 4.1min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.34043448631950163, score=0.338, total= 4.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 20.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.342, total= 2.9min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.380, total= 2.9min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.303, total= 2.9min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.416, total= 2.9min\n",
            "[CV] dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0 \n",
            "[CV]  dense_dropout=0.9, dense_layers=1, dense_units=1024, epochs=143, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.0, score=0.334, total= 2.9min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 14.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174, score=0.316, total=  13.5s\n",
            "[CV] dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174, score=0.306, total=  13.3s\n",
            "[CV] dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   26.8s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174, score=0.321, total=  13.7s\n",
            "[CV] dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174 \n",
            "[CV]  dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174, score=0.329, total=  13.5s\n",
            "[CV] dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174 \n",
            "[CV]  dense_dropout=0.38485749342124415, dense_layers=3, dense_units=1024, epochs=50, gru_dropout=0.0, gru_layers=1, gru_units=16, spatial_dropout=0.2963116592281174, score=0.302, total=  13.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9, score=0.166, total= 1.7min\n",
            "[CV] dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9, score=0.153, total= 1.7min\n",
            "[CV] dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9, score=0.156, total= 1.7min\n",
            "[CV] dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9 \n",
            "[CV]  dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9, score=0.170, total= 1.7min\n",
            "[CV] dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9 \n",
            "[CV]  dense_dropout=0.3004218585546998, dense_layers=2, dense_units=1024, epochs=200, gru_dropout=0.9, gru_layers=1, gru_units=512, spatial_dropout=0.9, score=0.176, total= 1.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049, score=0.302, total= 4.0min\n",
            "[CV] dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049, score=0.286, total= 3.9min\n",
            "[CV] dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049, score=0.301, total= 4.0min\n",
            "[CV] dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049 \n",
            "[CV]  dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049, score=0.308, total= 3.9min\n",
            "[CV] dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049 \n",
            "[CV]  dense_dropout=0.7192262624819056, dense_layers=1, dense_units=16, epochs=200, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.5275237074213049, score=0.315, total= 4.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 19.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724, score=0.347, total= 2.3min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724, score=0.310, total= 2.3min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724, score=0.289, total= 2.3min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724 \n",
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724, score=0.374, total= 2.3min\n",
            "[CV] dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724 \n",
            "[CV]  dense_dropout=0.0, dense_layers=1, dense_units=1024, epochs=110, gru_dropout=0.0, gru_layers=1, gru_units=1024, spatial_dropout=0.41227104827233724, score=0.338, total= 2.3min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 11.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 18min 28s, sys: 20min 29s, total: 2h 38min 57s\n",
            "Wall time: 6h 22min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPcDLchhrmj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce2322c-4fdf-42eb-a237-6de358166898"
      },
      "source": [
        "gru_bs_model.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('dense_dropout', 0.8390663304905417),\n",
              "             ('dense_layers', 1),\n",
              "             ('dense_units', 1024),\n",
              "             ('epochs', 200),\n",
              "             ('gru_dropout', 0.0),\n",
              "             ('gru_layers', 2),\n",
              "             ('gru_units', 1024),\n",
              "             ('spatial_dropout', 0.0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWtKaHApj6EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c727543-f2ef-41ab-b5bd-c944ec93b8a2"
      },
      "source": [
        "gru_bs_preds = gru_bs_model.predict(X_test)\r\n",
        "np.save('preds/gru_bs_preds', gru_bs_preds)\r\n",
        "print(classification_report(Y_test, gru_bs_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.71      0.63       173\n",
            "           1       0.29      0.19      0.23        31\n",
            "           2       0.50      0.07      0.12        14\n",
            "           3       0.71      0.64      0.68       210\n",
            "\n",
            "    accuracy                           0.62       428\n",
            "   macro avg       0.52      0.40      0.41       428\n",
            "weighted avg       0.62      0.62      0.61       428\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helN2jVRvfYF"
      },
      "source": [
        "Best params:\r\n",
        "\r\n",
        "      OrderedDict([('dense_dropout', 0.8390663304905417),\r\n",
        "                  ('dense_layers', 1),\r\n",
        "                  ('dense_units', 1024),\r\n",
        "                  ('epochs', 200),\r\n",
        "                  ('gru_dropout', 0.0),\r\n",
        "                  ('gru_layers', 2),\r\n",
        "                  ('gru_units', 1024),\r\n",
        "                  ('spatial_dropout', 0.0)])\r\n",
        "\r\n",
        "Best results:\r\n",
        "\r\n",
        "                    precision    recall  f1-score   support\r\n",
        "\r\n",
        "                0       0.56      0.71      0.63       173\r\n",
        "                1       0.29      0.19      0.23        31\r\n",
        "                2       0.50      0.07      0.12        14\r\n",
        "                3       0.71      0.64      0.68       210\r\n",
        "\r\n",
        "          accuracy                           0.62       428\r\n",
        "        macro avg       0.52      0.40      0.41       428\r\n",
        "      weighted avg       0.62      0.62      0.61       428\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23gaa8TKUdQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}